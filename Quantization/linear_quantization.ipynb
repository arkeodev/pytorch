{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear quantization is a process used to reduce the number of bits required to represent numbers in a model, thereby reducing the model size and speeding up computation. It involves mapping a continuous range of values (like floating-point numbers) to a discrete set of values (like integers). This is particularly useful in deploying models on resource-constrained devices where memory and compute power are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://raw.githubusercontent.com/arkeodev/pytorch-tutorial/main/Quantization/images/symmetric_vs_asymetric_linear_quantization.png\" width=\"300\" height=\"400\">\n",
    "    <figcaption>Symmetric vs Asymmetric Linear Quantization</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Linear Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Scale and Zero Point:**\n",
    "   - **Scale:** This is a factor used to map the floating-point range to the integer range. It determines the granularity of the quantized values.\n",
    "   - **Zero Point:** This is the integer value that corresponds to the zero value in the floating-point range. It allows the representation of negative numbers when using unsigned integers.\n",
    "\n",
    "2. **Quantization Formula:**\n",
    "   $$\n",
    "   Q_x = \\text{round}\\left(\\frac{x}{\\text{scale}} + \\text{zero\\_point}\\right)\n",
    "   $$\n",
    "   Here, $( x )$ is the original floating-point value, $( Q_x )$ is the quantized integer value, and the `round` function rounds the result to the nearest integer.\n",
    "\n",
    "3. **Dequantization Formula:**\n",
    "   $$\n",
    "   x = \\text{scale} \\times (Q_x - \\text{zero\\_point})\n",
    "   $$\n",
    "   This formula maps the quantized integer back to the original floating-point value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In symmetric quantization, the quantization range is symmetric around zero. This means that the range of positive and negative values is equal, and the zero point is typically zero. Symmetric quantization is simple and computationally efficient because the quantization and dequantization processes do not require adding a zero point.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  Q_x = \\text{round}\\left(\\frac{x}{\\text{scale}}\\right)\n",
    "  $$\n",
    "  $$\n",
    "  x = \\text{scale} \\times Q_x\n",
    "  $$\n",
    "\n",
    "- **Advantages:**\n",
    "  - Easier to implement and understand.\n",
    "  - Requires fewer calculations (no need for zero point adjustment).\n",
    "  - Often sufficient for weights in neural networks where the distribution is roughly symmetric around zero.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - May not be efficient for data where the distribution is not symmetric around zero, potentially leading to a loss of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In asymmetric quantization, the quantization range is not symmetric around zero. This allows for a better fit for data distributions that are skewed or do not center around zero. The zero point is a non-zero integer value, enabling the representation of both positive and negative ranges more effectively.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  Q_x = \\text{round}\\left(\\frac{x}{\\text{scale}} + \\text{zero\\_point}\\right)\n",
    "  $$\n",
    "  $$\n",
    "  x = \\text{scale} \\times (Q_x - \\text{zero\\_point})\n",
    "  $$\n",
    "\n",
    "- **Advantages:**\n",
    "  - More flexible and can handle a wider range of data distributions.\n",
    "  - Better suited for activations in neural networks where data can be non-symmetric and skewed.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - More complex to implement.\n",
    "  - Requires additional calculations for the zero point adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can implement symmetric and asymmetric quantization in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric Quantization\n",
      "Original tensor: tensor([ 1.2123,  2.3535, -1.1674, -2.4335,  0.5444, -0.3590])\n",
      "Quantized tensor: tensor([ 12.,  24., -12., -24.,   5.,  -4.])\n",
      "Dequantized tensor: tensor([ 1.2000,  2.4000, -1.2000, -2.4000,  0.5000, -0.4000])\n",
      "\n",
      "Asymmetric Quantization\n",
      "Original tensor: tensor([ 1.2123,  2.3535, -1.1674, -2.4335,  0.5444, -0.3590])\n",
      "Quantized tensor: tensor([140., 152., 116., 104., 133., 124.])\n",
      "Dequantized tensor: tensor([ 1.2000,  2.4000, -1.2000, -2.4000,  0.5000, -0.4000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "tensor = torch.tensor([1.2123, 2.3535, -1.1674, -2.4335, 0.5444, -0.3590])\n",
    "\n",
    "# Symmetric Quantization\n",
    "def symmetric_quantize(tensor, scale):\n",
    "    return torch.round(tensor / scale)\n",
    "\n",
    "def symmetric_dequantize(tensor, scale):\n",
    "    return tensor * scale\n",
    "\n",
    "scale = 0.1\n",
    "quantized_tensor_sym = symmetric_quantize(tensor, scale)\n",
    "dequantized_tensor_sym = symmetric_dequantize(quantized_tensor_sym, scale)\n",
    "\n",
    "print(\"Symmetric Quantization\")\n",
    "print(\"Original tensor:\", tensor)\n",
    "print(\"Quantized tensor:\", quantized_tensor_sym)\n",
    "print(\"Dequantized tensor:\", dequantized_tensor_sym)\n",
    "\n",
    "# Asymmetric Quantization\n",
    "def asymmetric_quantize(tensor, scale, zero_point):\n",
    "    return torch.round(tensor / scale + zero_point)\n",
    "\n",
    "def asymmetric_dequantize(tensor, scale, zero_point):\n",
    "    return scale * (tensor - zero_point)\n",
    "\n",
    "scale = 0.1\n",
    "zero_point = 128\n",
    "quantized_tensor_asym = asymmetric_quantize(tensor, scale, zero_point)\n",
    "dequantized_tensor_asym = asymmetric_dequantize(quantized_tensor_asym, scale, zero_point)\n",
    "\n",
    "print(\"\\nAsymmetric Quantization\")\n",
    "print(\"Original tensor:\", tensor)\n",
    "print(\"Quantized tensor:\", quantized_tensor_asym)\n",
    "print(\"Dequantized tensor:\", dequantized_tensor_asym)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
