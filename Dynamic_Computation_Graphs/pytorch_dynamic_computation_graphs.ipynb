{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/pytorch-tutorial/blob/main/pytorch_dynamic_computation_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML3Hf4ApUD4O"
      },
      "source": [
        "# Understanding Dynamic Computation Graphs in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Basics of Computation Graphs"
      ],
      "metadata": {
        "id": "QtC6GouCV52s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Computation Graph?"
      ],
      "metadata": {
        "id": "s2nfSeYIV-ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A computation graph is a directed graph where nodes represent mathematical operations or variables, and edges represent the flow of data between these operations. This graphical representation allows for an intuitive visualization of complex mathematical functions and their derivatives, making it a cornerstone of modern deep learning frameworks."
      ],
      "metadata": {
        "id": "MWJw8WIrWFAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Computation Graphs?"
      ],
      "metadata": {
        "id": "XAwwI1B8W1SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Simplification of Complex Operations: They break down complex operations into simpler steps.\n",
        "* Automatic Differentiation: They enable frameworks to automatically compute gradients—essential for training neural networks.\n",
        "* Optimization: They allow for optimization of mathematical expressions before execution."
      ],
      "metadata": {
        "id": "Eh9QPkzUW5UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: A Basic Computation Graph**\n",
        "\n",
        "Consider a simple neural network operation where we have an input $(x)$, which we multiply by a weight $(w)$ and then apply an activation function, such as a ReLU (Rectified Linear Unit). The operation can be expressed as $(y = ReLU(x \\cdot w))$."
      ],
      "metadata": {
        "id": "k-wl0vVIXVSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create tensors\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "w = torch.tensor([0.5, -0.5, 0.1], requires_grad=True)\n",
        "\n",
        "# Perform linear operation\n",
        "z = torch.dot(x, w)\n",
        "\n",
        "# Apply activation function (ReLU)\n",
        "y = F.relu(z)\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "print(f'Input x: {x}')\n",
        "print(f'Weights w: {w}')\n",
        "print(f'Output y after ReLU: {y}')\n",
        "print(f'Gradient of x: {x.grad}')\n",
        "print(f'Gradient of w: {w.grad}')"
      ],
      "metadata": {
        "id": "myUKsfTNXWLG",
        "outputId": "17a3b2ac-b5f9-4d70-9800-13f091dd6200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x: tensor([1., 2., 3.], requires_grad=True)\n",
            "Weights w: tensor([ 0.5000, -0.5000,  0.1000], requires_grad=True)\n",
            "Output y after ReLU: 0.0\n",
            "Gradient of x: tensor([0., -0., 0.])\n",
            "Gradient of w: tensor([0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "- `x` and `w` are input tensors with `requires_grad=True`, indicating that we want to compute gradients with respect to these tensors.\n",
        "- `torch.dot(x, w)` performs the linear operation (dot product).\n",
        "- `F.relu(z)` applies the ReLU activation function.\n",
        "- `y.backward()` computes the gradients of `y` with respect to the tensors that have `requires_grad=True`."
      ],
      "metadata": {
        "id": "Tk6yfu8MXrLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example where the graph is dynamically built during the execution of the operations.\n",
        "\n",
        "**The `backward()` call triggers the construction of the graph and the computation of the gradients, highlighting PyTorch's dynamic nature.**"
      ],
      "metadata": {
        "id": "RTV7iss6XBA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static vs. Dynamic Computation Graphs"
      ],
      "metadata": {
        "id": "ARzsORuPYK3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Static Computation Graphs"
      ],
      "metadata": {
        "id": "VrZo_SjjYxX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In static computation graphs, the structure of the network is defined and compiled before any actual numerical computation occurs. TensorFlow v1 is a prime example of a framework that utilizes static graphs.\n",
        "\n",
        "**Characteristics**:\n",
        "- **Predefined Structure**: The graph's architecture must be fully specified before runtime.\n",
        "- **Compilation Phase**: The graph is compiled into an optimized execution plan, which can lead to efficient execution on various hardware.\n",
        "- **Limited Flexibility**: Modifying the graph after compilation is not straightforward, requiring the entire graph to be redefined and recompiled for changes to take effect.\n",
        "\n",
        "**Example**: In TensorFlow v1, creating a static graph might involve defining placeholders for inputs, specifying operations, and then compiling the model for training and inference."
      ],
      "metadata": {
        "id": "deqryjEjY5IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Computation Graphs"
      ],
      "metadata": {
        "id": "_Xr1f3hPY8X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic computation graphs are constructed on-the-fly during execution. PyTorch is well-known for its use of dynamic graphs, also referred to as define-by-run graphs.\n",
        "\n",
        "**Characteristics**:\n",
        "- **Flexibility**: The graph's structure can change with every iteration, offering tremendous flexibility for models that need conditional operations or dynamic inputs.\n",
        "- **Ease of Debugging**: Since the graph is built at runtime, it's easier to debug. You can insert print statements or use debugging tools just as you would in any other Python script.\n",
        "- **Iterative Development**: Allows for more intuitive and interactive model development, as changes to the model's structure can be made on the fly.\n",
        "\n",
        "**Example**: In PyTorch, the computation graph for a model is created dynamically. This means you can use standard Python control flow operators like loops and conditionals to alter the graph each time the code is run."
      ],
      "metadata": {
        "id": "HviAJDSKZE01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Autograd in PyTorch"
      ],
      "metadata": {
        "id": "2Xmf0AVgaZ3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most powerful features of PyTorch is its `autograd` system. This system allows for automatic differentiation, making the training of neural networks straightforward and efficient. Let's dive into how `autograd` works and explore the role of the `Tensor` class and its `requires_grad` attribute."
      ],
      "metadata": {
        "id": "whL_NpVWaidA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Automatic Differentiation?"
      ],
      "metadata": {
        "id": "ZnB1eewzakcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic differentiation is a key component in training neural networks. It automates the computation of gradients—essential for the optimization of model parameters. In essence, automatic differentiation enables the adjustment of weights in response to the loss function, guiding the model towards better performance."
      ],
      "metadata": {
        "id": "3YZ8hzuza08n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### The Tensor Class and `requires_grad"
      ],
      "metadata": {
        "id": "VMNJZeqWa_DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Tensor` class is central to PyTorch. Tensors in PyTorch can store data (like weights and inputs in a network) and keep track of every operation applied to them. Here's where the `requires_grad` attribute comes into play:\n",
        "\n",
        "- **`requires_grad` Attribute**: When set to `True`, PyTorch tracks all operations on the tensor, and accumulates the gradients on another tensor (the `.grad` attribute) whenever the `.backward()` method is called. This is essential for learning parameters in neural networks."
      ],
      "metadata": {
        "id": "tuimeMqJbGtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Autograd Works"
      ],
      "metadata": {
        "id": "3O15_n5RbLlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When performing operations on tensors with `requires_grad=True`, PyTorch dynamically constructs a computation graph. Nodes in the graph represent tensors, while edges represent functions that produce output tensors from input tensors. When the `.backward()` method is called on a tensor (typically the loss), PyTorch calculates the gradients using this graph, propagating backwards from the tensor.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jq4gVLR9aO_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define inputs\n",
        "x = torch.randn(1, 10, requires_grad=True)  # A random tensor representing input features\n",
        "\n",
        "# Define weights and bias for a linear layer\n",
        "weights = torch.randn(10, 5, requires_grad=True)  # Weights for the linear layer\n",
        "bias = torch.randn(5, requires_grad=True)  # Bias for the linear layer\n",
        "\n",
        "# Linear operation\n",
        "linear_output = torch.matmul(x, weights) + bias\n",
        "\n",
        "# Activation function (ReLU)\n",
        "relu_output = F.relu(linear_output)\n",
        "\n",
        "# Compute gradients\n",
        "relu_output.backward(torch.ones_like(relu_output))\n",
        "\n",
        "print(f'Gradients for weights: \\n{weights.grad}')"
      ],
      "metadata": {
        "id": "7vJg7JaOc3VX",
        "outputId": "3216f21b-1590-403a-c344-d6472e921608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for weights: \n",
            "tensor([[-0.2069, -0.2069, -0.2069, -0.2069, -0.0000],\n",
            "        [-0.8820, -0.8820, -0.8820, -0.8820, -0.0000],\n",
            "        [ 2.6136,  2.6136,  2.6136,  2.6136,  0.0000],\n",
            "        [ 2.7139,  2.7139,  2.7139,  2.7139,  0.0000],\n",
            "        [-1.6857, -1.6857, -1.6857, -1.6857, -0.0000],\n",
            "        [-1.4667, -1.4667, -1.4667, -1.4667, -0.0000],\n",
            "        [-1.3647, -1.3647, -1.3647, -1.3647, -0.0000],\n",
            "        [-0.1918, -0.1918, -0.1918, -0.1918, -0.0000],\n",
            "        [ 0.7730,  0.7730,  0.7730,  0.7730,  0.0000],\n",
            "        [-0.0225, -0.0225, -0.0225, -0.0225, -0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation in Dynamic Graphs"
      ],
      "metadata": {
        "id": "Hl9i33rydyzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Backpropagation in the context of dynamic graphs involves three main steps:\n",
        "\n",
        "1. **Forward Pass**: In this step, the network makes predictions by performing operations on the input data, constructing the computation graph in the process. Each node in the graph represents an operation, and each edge represents a tensor that flows between operations.\n",
        "\n",
        "2. **Loss Calculation**: Once the output is obtained, the loss (or error) is computed using a loss function. This loss indicates how far off the predictions are from the actual values.\n",
        "\n",
        "3. **Backward Pass**: During the backward pass, gradients of the loss function with respect to each parameter (weight) are calculated by traversing the graph in reverse. This is where the dynamic nature of PyTorch shines; the graph can adapt to each iteration's operations, making it straightforward to compute gradients even in complex models.\n"
      ],
      "metadata": {
        "id": "LyvkJ0tFdncv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Graph Modifications During Runtime"
      ],
      "metadata": {
        "id": "KhreJQzjfw9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most powerful features of dynamic computation graphs, as utilized in PyTorch, is the ability to modify the graph during runtime. This capability allows for a level of flexibility and adaptability in model architecture that is particularly useful in certain deep learning scenarios.\n"
      ],
      "metadata": {
        "id": "2Nh_Nt64f2M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying the computation graph during runtime can be essential for a variety of reasons:"
      ],
      "metadata": {
        "id": "OO1EaoI3f7bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Conditional Operations"
      ],
      "metadata": {
        "id": "E4ijIQQ5f_MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain models may require different computation paths based on runtime data. For instance, in decision-making models or models that process variable-length sequences, the operations performed may vary from one input to another."
      ],
      "metadata": {
        "id": "C3M1ld9bgFcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simplified example demonstrating how a conditional operation can modify the computation graph in PyTorch based on runtime data. This example simulates a scenario where an additional layer is applied to the input if its sum exceeds a certain threshold:"
      ],
      "metadata": {
        "id": "9RlL6FlzgL9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConditionalModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(10, 20)\n",
        "        self.linear2 = nn.Linear(20, 5)\n",
        "        self.threshold = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        # Conditional operation based on runtime data\n",
        "        if x.sum() > self.threshold:\n",
        "            x = F.relu(self.linear2(x))\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = ConditionalModel()\n",
        "\n",
        "# Example input tensor\n",
        "input_tensor = torch.randn(1, 10)\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "kwQwUfnLgPAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example:\n",
        "- We define a simple neural network model with two linear layers.\n",
        "- During the forward pass, the input tensor is passed through the first linear layer and a ReLU activation function.\n",
        "- A conditional check is performed on the sum of the output from the first layer. If the sum exceeds a predefined threshold, the output is passed through a second linear layer and another ReLU activation. This conditionally modifies the computation graph based on the runtime data.\n",
        "- The flexibility of dynamic graphs in PyTorch allows for this type of runtime modification to the model's structure, which would be much more complex or even impossible in a static graph framework."
      ],
      "metadata": {
        "id": "0zUC9GkwgVGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Network Architectures"
      ],
      "metadata": {
        "id": "KqYOg97BgXqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some advanced neural network architectures adjust their structure during training to optimize performance, requiring dynamic adjustments to the computation graph.\n",
        "\n",
        "- **Recurrent Neural Networks (RNNs)** dealing with variable sequence lengths where the computation graph needs to adapt to the length of each input sequence.\n",
        "- **Neural Networks with Conditional Logic**, where certain layers or operations are only applied based on specific conditions met by the input data.\n",
        "- **Neuroevolutionary Models** where the architecture of the network itself is subject to change and evolution as part of the training process."
      ],
      "metadata": {
        "id": "tWJ_tH0WgbGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: RNNs for Variable Sequence Lengths\n",
        "\n",
        "Let's consider a simple example illustrating how PyTorch's dynamic graphs facilitate working with variable sequence lengths in RNNs:"
      ],
      "metadata": {
        "id": "XW5lxHCEg_3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VariableLengthRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(VariableLengthRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pack the sequence\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output_packed, _ = self.rnn(x_packed)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "model = VariableLengthRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Dummy input tensor (batch size, sequence length, input size)\n",
        "x = torch.randn(3, 5, 10)\n",
        "lengths = torch.tensor([5, 3, 2])  # Actual lengths of each sequence in the batch\n",
        "\n",
        "output = model(x, lengths)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "u-gv_z8chD4R",
        "outputId": "6c00b200-df5a-4aee-ad50-f7905fc57702",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2228,  0.2660, -0.3310,  0.5142, -0.1781],\n",
            "         [-0.2058, -0.0255, -0.0986,  0.5551,  0.2771],\n",
            "         [ 0.0232,  0.5068, -0.4271,  0.2033, -0.1650],\n",
            "         [-0.0571,  0.2920, -0.1142,  0.4609, -0.1046],\n",
            "         [-0.1845,  0.0746,  0.1271,  0.2061, -0.0874]],\n",
            "\n",
            "        [[-0.2280,  0.1238, -0.1963,  0.2903,  0.0523],\n",
            "         [-0.2003,  0.2125, -0.3221,  0.4434, -0.1058],\n",
            "         [-0.1425,  0.2660, -0.4307,  0.5038,  0.1863],\n",
            "         [-0.1296,  0.1775, -0.1950,  0.1877, -0.0291],\n",
            "         [-0.1296,  0.1775, -0.1950,  0.1877, -0.0291]],\n",
            "\n",
            "        [[-0.0624,  0.6285, -0.4277,  0.4021, -0.3546],\n",
            "         [-0.1996,  0.1118, -0.0439,  0.3617,  0.2235],\n",
            "         [-0.1296,  0.1775, -0.1950,  0.1877, -0.0291],\n",
            "         [-0.1296,  0.1775, -0.1950,  0.1877, -0.0291],\n",
            "         [-0.1296,  0.1775, -0.1950,  0.1877, -0.0291]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the VariableLengthRNN model dynamically handles input sequences of varying lengths, illustrating the flexibility of dynamic graphs in PyTorch. The model can process each sequence according to its actual length, optimizing performance and efficiency."
      ],
      "metadata": {
        "id": "14md52CHhKRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trade-offs and Considerations of Dynamic Computation Graphs"
      ],
      "metadata": {
        "id": "xOLW8Ku7h5Tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While dynamic computation graphs offer significant advantages in flexibility and ease of use, there are trade-offs and considerations that practitioners should be aware of. Understanding these can help in making informed decisions and optimizing performance when working with frameworks like PyTorch."
      ],
      "metadata": {
        "id": "rSCO_a7wh-o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Potential Downsides of Dynamic Graphs"
      ],
      "metadata": {
        "id": "s5IZALI9iAoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Runtime Overhead**: Since the graph is built and rebuilt at each iteration, there can be additional overhead compared to static graphs, where the computation graph is built once and then executed efficiently. This can impact training speed, especially for very large models or datasets.\n",
        "\n",
        "2. **Memory Usage**: Dynamic graphs can sometimes use more memory because the framework needs to store additional information at runtime to build the graph dynamically. This can be a consideration when working with large models or limited hardware resources.\n",
        "\n",
        "3. **Optimization Limitations**: Static graphs allow for extensive optimization before execution, including simplifying calculations, fusing operations, and optimizing memory usage. Dynamic graphs, due to their nature, might not benefit from these optimizations to the same extent."
      ],
      "metadata": {
        "id": "yx7WZ6GZiD38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing Performance with Dynamic Graphs"
      ],
      "metadata": {
        "id": "El3CIK0HiHp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite these considerations, there are several strategies and best practices to mitigate potential downsides and optimize performance when using dynamic graphs:"
      ],
      "metadata": {
        "id": "A4vcgpE1iQ6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Use TorchScript**: TorchScript is a tool in PyTorch that allows for the creation of serializable and optimizable models from PyTorch code. By converting a dynamic model to TorchScript, you can gain performance improvements similar to those of static graphs while retaining the flexibility of PyTorch.\n",
        "\n",
        "    ```python\n",
        "    import torch\n",
        "    class MyModel(torch.nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * 2\n",
        "\n",
        "    model = MyModel()\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    ```\n",
        "\n",
        "2. **Optimize Data Loading**: The efficiency of your data pipeline can significantly affect overall performance. Utilize `torch.utils.data.DataLoader` with multiple workers (`num_workers`) to parallelize data loading and preprocessing.\n",
        "\n",
        "3. **Profile Your Models**: PyTorch provides profiling tools that can help identify bottlenecks in your models. Use `torch.autograd.profiler` to understand where the most time is being spent and optimize those areas.\n",
        "\n",
        "    ```python\n",
        "    with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "        # Your model inference or training loop here\n",
        "    print(prof)\n",
        "    ```\n",
        "\n",
        "4. **Minimize Tensor Operations Inside Loops**: Operations inside loops can dramatically slow down your training or inference time. Whenever possible, try to use batch operations outside loops and leverage PyTorch’s optimized tensor operations.\n",
        "\n",
        "5. **Leverage In-Place Operations**: In-place operations in PyTorch (those that end with `_`, like `add_`) can reduce memory usage by modifying tensors in-place rather than creating new ones. However, use them cautiously as they can make debugging more difficult.\n",
        "\n",
        "6. **Manage Device Memory**: When working with GPU, efficiently manage memory by moving tensors to the device at the beginning of your workflow and ensuring that intermediate tensors are deleted or go out of scope when no longer needed."
      ],
      "metadata": {
        "id": "FUJEqhZah2RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zeroing vs. Accumulating Gradients"
      ],
      "metadata": {
        "id": "k8hXLIYMiwp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the training loop of a neural network, managing gradients properly is crucial for effective learning. Understanding when to accumulate gradients and when to zero them out is key to optimizing your training process"
      ],
      "metadata": {
        "id": "ZNuu62uHi4AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Zero the Gradients"
      ],
      "metadata": {
        "id": "l0o8uAHMjAcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Start of Each Iteration**: Typically, you should zero the gradients at the beginning of each training iteration. This is because gradients are accumulated by default whenever `.backward()` is called on the loss tensor. If you don't manually zero the gradients, you'll end up with accumulated gradients from all previous `.backward()` calls, which leads to incorrect gradient values during optimization.\n",
        "\n",
        "\n",
        "    ```python\n",
        "    optimizer.zero_grad()\n",
        "    ```\n",
        "\n",
        "    This command zeroes the gradients of all parameters in the optimizer, ensuring that only the current iteration's gradients are used for parameter updates."
      ],
      "metadata": {
        "id": "4zsaOxdijSOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Accumulate Gradients"
      ],
      "metadata": {
        "id": "89utDDdMjUPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Handling Large Datasets/Batches**: Sometimes, the dataset or batch size is too large to fit into memory. In such cases, you might divide your data into smaller mini-batches that fit into memory but still want to update the model as if the entire batch was processed together. This technique is known as gradient accumulation.\n",
        "\n",
        "    Here, you intentionally skip zeroing out the gradients for several iterations, allowing the gradients to accumulate from multiple mini-batches. After accumulating the desired amount, you perform a single optimization step.\n",
        "\n",
        "    ```python\n",
        "    # Assume n_steps is the number of steps for which you want to accumulate gradients\n",
        "    for step, (inputs, labels) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        if (step + 1) % n_steps == 0:\n",
        "            # Update parameters after accumulating gradients\n",
        "            optimizer.step()\n",
        "            # Zero the gradients after updating\n",
        "            optimizer.zero_grad()\n",
        "    ```\n",
        "\n",
        "    In this scenario, gradients from `n_steps` mini-batches are accumulated before a single optimization step is performed. This approach allows the effective batch size to be larger than what could physically fit in memory, improving model performance without increasing hardware requirements.\n",
        "\n"
      ],
      "metadata": {
        "id": "OFRL9u4Siu8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Advanced Uses of Autograd"
      ],
      "metadata": {
        "id": "T3tSEUjsj2yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch's `autograd` system offers versatility far beyond the basic gradient calculations required for backpropagation in neural network training. It supports the computation of higher-order derivatives and the automatic differentiation of arbitrary functions, making it a powerful tool for a wide range of applications in optimization, scientific computing, and beyond. Let's delve into these capabilities:"
      ],
      "metadata": {
        "id": "is0csbqYkGjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Higher-Order Derivatives"
      ],
      "metadata": {
        "id": "dBgc0s9ckNy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher-order derivatives are derivatives of derivatives, providing deeper insights into the behavior of functions, such as curvature and stability analysis. In the context of deep learning, second-order derivatives (Hessians) can be used in advanced optimization methods, including Newton's method, which requires the Hessian matrix for updates.\n",
        "\n",
        "PyTorch's `autograd` can compute higher-order derivatives by applying the `.backward()` method multiple times, given that the graph is retained. For second-order derivatives, one can use the `torch.autograd.functional.hessian` function to directly compute the Hessian matrix."
      ],
      "metadata": {
        "id": "MrSmsavtkS5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a simple function\n",
        "def f(x):\n",
        "    return x ** 4 - x ** 2\n",
        "\n",
        "# Create a tensor and enable gradient computation\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# First derivative\n",
        "y = f(x)\n",
        "y.backward(create_graph=True)\n",
        "\n",
        "# Second derivative (Hessian for this scalar function)\n",
        "x_grad = x.grad\n",
        "x_grad.backward()\n",
        "hessian = x.grad\n",
        "\n",
        "print(f\"First derivative at x=1: {x_grad}\")\n",
        "print(f\"Second derivative (Hessian) at x=1: {hessian}\")"
      ],
      "metadata": {
        "id": "Uva10E6vkxAA",
        "outputId": "ff887d6f-912e-4d80-fab7-521ac87f7012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First derivative at x=1: tensor([12.], grad_fn=<CopyBackwards>)\n",
            "Second derivative (Hessian) at x=1: tensor([12.], grad_fn=<CopyBackwards>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Differentiation of Arbitrary Functions"
      ],
      "metadata": {
        "id": "Yit_jK3lkzSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic differentiation (AD) in PyTorch is not limited to neural networks or machine learning models. It can be applied to any differentiable function coded up in PyTorch. This makes it invaluable for tasks requiring gradient computation across various fields, including physics simulations, engineering design optimization, and financial modeling.\n",
        "\n",
        "AD works by decomposing complex functions into elementary operations for which derivatives are known. The chain rule is then applied to these operations to compute the derivative of the entire function. PyTorch handles this process dynamically, constructing the computation graph on-the-fly."
      ],
      "metadata": {
        "id": "PCsEP4hfk2e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define an arbitrary differentiable function\n",
        "def g(x, y):\n",
        "    return torch.sin(x) * torch.log(y)\n",
        "\n",
        "# Variables with requires_grad=True\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "# Compute the function\n",
        "z = g(x, y)\n",
        "\n",
        "# Compute gradients\n",
        "z.backward()\n",
        "\n",
        "print(f\"Gradient with respect to x: {x.grad}\")\n",
        "print(f\"Gradient with respect to y: {y.grad}\")"
      ],
      "metadata": {
        "id": "aCxad06jk9_L",
        "outputId": "572f88b1-ecf6-4d9b-a93d-50388e98ea06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient with respect to x: tensor([0.3745])\n",
            "Gradient with respect to y: tensor([0.4207])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Application of Autograd to the Realm of Reinforcement Learning (RL)"
      ],
      "metadata": {
        "id": "VAiADJGMmQzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The application of PyTorch's `autograd` system extends into the realm of Reinforcement Learning (RL), where it provides a powerful mechanism for learning policies that maximize cumulative rewards. In RL, an agent learns to make decisions by interacting with an environment to achieve a goal. The agent's ability to learn and improve over time hinges on its capacity to evaluate the effectiveness of its actions and adjust its strategy accordingly. This is where `autograd` comes into play, facilitating the optimization of policy parameters based on the computed gradients of the reward function."
      ],
      "metadata": {
        "id": "XaaF783kmpLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reinforcement Learning Basics"
      ],
      "metadata": {
        "id": "bI4ecyEMmsFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In RL, the agent follows a policy (π) to decide on actions (a) based on the current state (s) of the environment. The policy is often parameterized by θ, and the agent's objective is to maximize the expected cumulative reward. The reward signal is a critical feedback mechanism that indicates the effectiveness of the chosen actions. By applying gradient-based optimization, the agent iteratively updates its policy parameters (θ) to improve performance."
      ],
      "metadata": {
        "id": "pLsCsrjDmvRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Autograd in RL"
      ],
      "metadata": {
        "id": "_1pJOpD5m4w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of updating the policy parameters requires the computation of gradients of the expected cumulative reward with respect to the policy parameters (θ). This is where PyTorch's `autograd` becomes invaluable. It automates the computation of these gradients, even when the reward function is complex and involves a long sequence of actions and states."
      ],
      "metadata": {
        "id": "r98-UMJWm_Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Policy Gradient Methods"
      ],
      "metadata": {
        "id": "YIHTo6zxnBK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy gradient methods are a class of algorithms in RL that directly optimize the policy parameters by computing estimates of the gradients of the expected reward. The `autograd` system is particularly well-suited for implementing these algorithms, as it can efficiently compute gradients through sequences of operations, which in RL corresponds to sequences of states and actions."
      ],
      "metadata": {
        "id": "17XKrE8MnH7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Simple Policy Gradient"
      ],
      "metadata": {
        "id": "Yg3pYcv7nMk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simplified conceptual example to illustrate how `autograd` might be used in an RL context to update policy parameters:"
      ],
      "metadata": {
        "id": "mGT8snOpnPY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define necessary variables\n",
        "features = 10  # Number of input features\n",
        "actions = 2    # Number of output actions\n",
        "learning_rate = 0.01\n",
        "num_episodes = 5  # Number of episodes for training\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.linear = nn.Linear(features, actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.linear(x), dim=-1)\n",
        "\n",
        "# Initialize policy\n",
        "policy = Policy()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n",
        "# Dummy data for the example\n",
        "states = torch.randn(num_episodes, features)\n",
        "\n",
        "# Assume these are cumulative rewards for simplicity\n",
        "rewards = torch.randn(num_episodes)\n",
        "\n",
        "for state, reward in zip(states, rewards):\n",
        "    # Forward pass to get action probabilities\n",
        "    action_probs = policy(state.unsqueeze(0))  # Add batch dimension\n",
        "\n",
        "    # Dummy action selection (for illustration)\n",
        "    selected_action = torch.argmax(action_probs)\n",
        "     # Remove batch dimension and get the selected action probability\n",
        "    selected_action_prob = action_probs.squeeze()[selected_action]\n",
        "\n",
        "    # Simplified loss function for illustration\n",
        "    loss = -torch.log(selected_action_prob) * reward\n",
        "\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward()\n",
        "    # Update policy parameters\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "XJSO3aTpnZcF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this corrected script:\n",
        "- `features` and `actions` are defined to specify the input and output sizes of the linear layer in the `Policy` model.\n",
        "- `num_episodes` is defined to simulate the number of training episodes.\n",
        "- A batch dimension is added to the state tensor before passing it to the model by using `unsqueeze(0)`. This is important because PyTorch models expect input data in the format `(batch_size, features)`.\n",
        "- The selection of an action and the calculation of the loss have been simplified to fit the context of this example. It demonstrates selecting the action with the highest probability and using its probability to compute the loss."
      ],
      "metadata": {
        "id": "4cCiJ0ZqmKkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How PyTorch Manages Memory Automatically"
      ],
      "metadata": {
        "id": "GLYD8DcepLZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch's `autograd` system plays a crucial role in optimizing memory usage during the training process, ensuring resources are efficiently allocated and freed up as needed."
      ],
      "metadata": {
        "id": "RJRb_ft9pSqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how it contributes to automatic memory management:\n",
        "\n",
        "- **Tracking Tensor Dependencies**: Every tensor operation adds nodes to the computation graph, creating a network of dependencies among tensors. This network is key to understanding which tensors are necessary for computing gradients and which can be discarded.\n",
        "  \n",
        "- **Freeing Unused Tensors**: During the backward pass, once gradients are computed for a given tensor, PyTorch can automatically release the memory of tensors that are no longer needed.\n",
        "\n",
        "- **Delayed Allocation**: The dynamic nature of PyTorch's computation graph means memory for gradients doesn’t need to be allocated until it's clear what the memory requirements will be. This just-in-time approach to memory allocation allows for more efficient use of available resources."
      ],
      "metadata": {
        "id": "IZiR8xPhpa5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While PyTorch's automatic memory management handles much of the heavy lifting, users can optimize memory usage further by:\n",
        "\n",
        "- **Using In-Place Operations**: Where appropriate, use in-place versions of operations (e.g., `add_()` instead of `add()`) to reduce memory footprint.\n",
        "  \n",
        "- **Detaching Tensors**: If you have tensors that you know will not be needed for gradient computation, use `.detach()` to remove them from the computation graph and reduce memory usage.\n",
        "\n",
        "- **Clearing the Cache in CUDA**: For GPU training, periodically clearing the CUDA cache with `torch.cuda.empty_cache()` can help free up unused memory."
      ],
      "metadata": {
        "id": "b6-Jiu4hpwmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of the Computational Graph"
      ],
      "metadata": {
        "id": "-FSiFGVnqbIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization tools that integrate with PyTorch can render the computation graph, providing insights into the model's structure and the relationships between its components. This can be invaluable for:"
      ],
      "metadata": {
        "id": "u0iioIqxqd1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several libraries and tools are available for visualizing PyTorch computation graphs, each offering different features and levels of detail:\n",
        "\n",
        "- **TensorBoard**: Originally developed for TensorFlow, TensorBoard can be used with PyTorch via the `torch.utils.tensorboard` module. It provides visualization for the computational graph, along with training metrics and model parameters.\n",
        "  \n",
        "- **torchviz**: A lightweight tool specifically designed for visualizing PyTorch computation graphs. It generates a graph visualization using the `Graphviz` software, highlighting the operations and tensors involved in the model's forward pass.\n",
        "\n",
        "- **Netron**: While primarily used for visualizing ONNX models, Netron can also display models converted from PyTorch to the ONNX format. This can be useful for a high-level overview of the model architecture."
      ],
      "metadata": {
        "id": "BygvgwS5qrdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a simple example of how to use `torchviz` to visualize a computation graph of a PyTorch model:"
      ],
      "metadata": {
        "id": "zF3H6So5qwg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchviz -q"
      ],
      "metadata": {
        "id": "JhAo_FY3q32d",
        "outputId": "98d426fb-a510-4b7a-e8f1-4be882ab7187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Define a simple model\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(2, 1)\n",
        ")\n",
        "\n",
        "# Generate a dummy input\n",
        "x = torch.randn(1, 2)\n",
        "\n",
        "# Perform a forward pass\n",
        "y = model(x)\n",
        "\n",
        "# Visualize the computation graph\n",
        "make_dot(y, params=dict(model.named_parameters()))"
      ],
      "metadata": {
        "id": "29bXEANRq0JW",
        "outputId": "efb3eaa5-528a-4208-c40f-5f1d715d3a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"340pt\" height=\"413pt\"\n viewBox=\"0.00 0.00 340.00 413.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 409)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-409 336,-409 336,4 -4,4\"/>\n<!-- 138577659509344 -->\n<g id=\"node1\" class=\"node\">\n<title>138577659509344</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"196,-31 137,-31 137,0 196,0 196,-31\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 1)</text>\n</g>\n<!-- 138577275021696 -->\n<g id=\"node2\" class=\"node\">\n<title>138577275021696</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-86 116,-86 116,-67 217,-67 217,-86\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 138577275021696&#45;&gt;138577659509344 -->\n<g id=\"edge13\" class=\"edge\">\n<title>138577275021696&#45;&gt;138577659509344</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-66.79C166.5,-60.07 166.5,-50.4 166.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-41.19 166.5,-31.19 163,-41.19 170,-41.19\"/>\n</g>\n<!-- 138577275025776 -->\n<g id=\"node3\" class=\"node\">\n<title>138577275025776</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138577275025776&#45;&gt;138577275021696 -->\n<g id=\"edge1\" class=\"edge\">\n<title>138577275025776&#45;&gt;138577275021696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M69.14,-121.98C87.8,-113.46 116.75,-100.23 138.24,-90.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"139.88,-93.51 147.52,-86.17 136.97,-87.14 139.88,-93.51\"/>\n</g>\n<!-- 138577274640624 -->\n<g id=\"node4\" class=\"node\">\n<title>138577274640624</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-207 23.5,-207 23.5,-177 77.5,-177 77.5,-207\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">2.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 138577274640624&#45;&gt;138577275025776 -->\n<g id=\"edge2\" class=\"edge\">\n<title>138577274640624&#45;&gt;138577275025776</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n</g>\n<!-- 138577275025584 -->\n<g id=\"node5\" class=\"node\">\n<title>138577275025584</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-141 119,-141 119,-122 214,-122 214,-141\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 138577275025584&#45;&gt;138577275021696 -->\n<g id=\"edge3\" class=\"edge\">\n<title>138577275025584&#45;&gt;138577275021696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-121.75C166.5,-114.8 166.5,-104.85 166.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-96.09 166.5,-86.09 163,-96.09 170,-96.09\"/>\n</g>\n<!-- 138577275025536 -->\n<g id=\"node6\" class=\"node\">\n<title>138577275025536</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-201.5 112,-201.5 112,-182.5 213,-182.5 213,-201.5\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 138577275025536&#45;&gt;138577275025584 -->\n<g id=\"edge4\" class=\"edge\">\n<title>138577275025536&#45;&gt;138577275025584</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.09,-182.37C163.65,-174.25 164.5,-161.81 165.21,-151.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"168.72,-151.38 165.91,-141.17 161.73,-150.91 168.72,-151.38\"/>\n</g>\n<!-- 138577275020544 -->\n<g id=\"node7\" class=\"node\">\n<title>138577275020544</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"121,-267.5 20,-267.5 20,-248.5 121,-248.5 121,-267.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138577275020544&#45;&gt;138577275025536 -->\n<g id=\"edge5\" class=\"edge\">\n<title>138577275020544&#45;&gt;138577275025536</title>\n<path fill=\"none\" stroke=\"black\" d=\"M82.91,-248.37C97.86,-237.97 123.22,-220.32 141.3,-207.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"143.78,-210.28 150,-201.7 139.79,-204.54 143.78,-210.28\"/>\n</g>\n<!-- 138577261684400 -->\n<g id=\"node8\" class=\"node\">\n<title>138577261684400</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"97.5,-339 43.5,-339 43.5,-309 97.5,-309 97.5,-339\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\">0.bias</text>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n</g>\n<!-- 138577261684400&#45;&gt;138577275020544 -->\n<g id=\"edge6\" class=\"edge\">\n<title>138577261684400&#45;&gt;138577275020544</title>\n<path fill=\"none\" stroke=\"black\" d=\"M70.5,-308.8C70.5,-299.7 70.5,-287.79 70.5,-277.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"74,-277.84 70.5,-267.84 67,-277.84 74,-277.84\"/>\n</g>\n<!-- 138577275026208 -->\n<g id=\"node9\" class=\"node\">\n<title>138577275026208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"216,-267.5 139,-267.5 139,-248.5 216,-248.5 216,-267.5\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 138577275026208&#45;&gt;138577275025536 -->\n<g id=\"edge7\" class=\"edge\">\n<title>138577275026208&#45;&gt;138577275025536</title>\n<path fill=\"none\" stroke=\"black\" d=\"M175.48,-248.37C173.3,-239.07 169.76,-223.98 166.93,-211.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170.28,-210.84 164.59,-201.91 163.46,-212.44 170.28,-210.84\"/>\n</g>\n<!-- 138577275020496 -->\n<g id=\"node10\" class=\"node\">\n<title>138577275020496</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"228,-333.5 127,-333.5 127,-314.5 228,-314.5 228,-333.5\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138577275020496&#45;&gt;138577275026208 -->\n<g id=\"edge8\" class=\"edge\">\n<title>138577275020496&#45;&gt;138577275026208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M177.5,-314.37C177.5,-305.16 177.5,-290.29 177.5,-278.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181,-277.91 177.5,-267.91 174,-277.91 181,-277.91\"/>\n</g>\n<!-- 138577261685360 -->\n<g id=\"node11\" class=\"node\">\n<title>138577261685360</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"210,-405 145,-405 145,-375 210,-375 210,-405\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\">0.weight</text>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\"> (2, 2)</text>\n</g>\n<!-- 138577261685360&#45;&gt;138577275020496 -->\n<g id=\"edge9\" class=\"edge\">\n<title>138577261685360&#45;&gt;138577275020496</title>\n<path fill=\"none\" stroke=\"black\" d=\"M177.5,-374.8C177.5,-365.7 177.5,-353.79 177.5,-343.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181,-343.84 177.5,-333.84 174,-343.84 181,-343.84\"/>\n</g>\n<!-- 138577275025680 -->\n<g id=\"node12\" class=\"node\">\n<title>138577275025680</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"316,-141 239,-141 239,-122 316,-122 316,-141\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 138577275025680&#45;&gt;138577275021696 -->\n<g id=\"edge10\" class=\"edge\">\n<title>138577275025680&#45;&gt;138577275021696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M259.67,-121.98C241.89,-113.5 214.35,-100.35 193.82,-90.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"195.19,-87.32 184.66,-86.17 192.17,-93.64 195.19,-87.32\"/>\n</g>\n<!-- 138577275020208 -->\n<g id=\"node13\" class=\"node\">\n<title>138577275020208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"332,-201.5 231,-201.5 231,-182.5 332,-182.5 332,-201.5\"/>\n<text text-anchor=\"middle\" x=\"281.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138577275020208&#45;&gt;138577275025680 -->\n<g id=\"edge11\" class=\"edge\">\n<title>138577275020208&#45;&gt;138577275025680</title>\n<path fill=\"none\" stroke=\"black\" d=\"M280.91,-182.37C280.35,-174.25 279.5,-161.81 278.79,-151.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"282.27,-150.91 278.09,-141.17 275.28,-151.38 282.27,-150.91\"/>\n</g>\n<!-- 138577659381696 -->\n<g id=\"node14\" class=\"node\">\n<title>138577659381696</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"314,-273 249,-273 249,-243 314,-243 314,-273\"/>\n<text text-anchor=\"middle\" x=\"281.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">2.weight</text>\n<text text-anchor=\"middle\" x=\"281.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (1, 2)</text>\n</g>\n<!-- 138577659381696&#45;&gt;138577275020208 -->\n<g id=\"edge12\" class=\"edge\">\n<title>138577659381696&#45;&gt;138577275020208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281.5,-242.8C281.5,-233.7 281.5,-221.79 281.5,-211.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"285,-211.84 281.5,-201.84 278,-211.84 285,-211.84\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7e0909407850>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will generate a visualization of the computation graph, showing how the dummy input `x` flows through the model to produce the output `y`, along with the parameters of each layer."
      ],
      "metadata": {
        "id": "PE4pYtkqqNoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "g7IK7_Joby0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Overview of PyTorch Autograd Engine: (https://pytorch.org/blog/overview-of-pytorch-autograd-engine/)\n",
        "\n",
        "- How Computational Graphs are Constructed in PyTorch: (https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/)"
      ],
      "metadata": {
        "id": "0B3SoPLwb1WK"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}