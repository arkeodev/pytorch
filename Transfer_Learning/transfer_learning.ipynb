{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/pytorch-tutorial/blob/main/Transfer_Learning/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEeIjTpfQFe2"
      },
      "source": [
        "$$\n",
        "\\begin{array}{c}\n",
        "\\text{$\\Large If\\ I\\ have\\ seen\\ further;\\ it\\ is\\ by\\ standing\\ on\\ the\\ shoulders\\ of\\ Giants.$} \\\\\n",
        "{\\text{{$\\small Isaac\\ Newton$}}} \\\\\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImIZUq1ZPzGH"
      },
      "source": [
        "# Unlocking the Power of Pretrained Models with PyTorch Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hxTGQvuP6DA"
      },
      "source": [
        "In the machine learning and deep learning, building a model from scratch to solve complex problems can be a daunting task. It requires substantial data, immense computational resources, and considerable time. However, what if you could stand on the shoulders of giants and leverage what has already been learned? This is where transfer learning offering a shortcut to advanced deep learning capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gTdWSuvy62A"
      },
      "source": [
        "## Types of Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LTxcOBzOov"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/pytorch-tutorial/main/Transfer_Learning/images/different_types_of_learning.png\" width=\"900\" height=\"400\" alt=\"Types of Learning\">\n",
        "    <figcaption>Types of Learning</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwDxBa_VzRIr"
      },
      "source": [
        "### (a) Original Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiwXKqJ0zBvh"
      },
      "source": [
        "\n",
        "The original model, typically a neural network, has been trained on a set of tasks (e.g., task 1 to task m). This model serves as a starting point and has a set of parameters $\\theta_s$ for shared layers and $\\theta_o$ for task-specific layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8ix3iRzTCZ"
      },
      "source": [
        "### (b) Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u-_613JzDg1"
      },
      "source": [
        "\n",
        "Fine-tuning involves taking a pre-trained model (on one or more tasks) and continuing the training process to adapt the model to a new task. Here, the shared layers may be frozen or continue to learn at a lower rate (shown in blue), while the new task-specific layers (shown in orange) are trained from scratch or fine-tuned at a higher rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bhQL84KzVXA"
      },
      "source": [
        "### (c) Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JckRIme3zF2Z"
      },
      "source": [
        "\n",
        "Feature extraction keeps the pre-trained layers unchanged and only trains a new set of layers for the new task. The shared layers act as a fixed feature extractor, and only the parameters for the new task are learned (orange part), often using a shallower network than the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6R_XZdzXq3"
      },
      "source": [
        "### (d) Joint Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mvqXQHzIWX"
      },
      "source": [
        "\n",
        "Joint training simultaneously trains the model on the original tasks and the new task. This approach can help prevent catastrophic forgetting by ensuring that the model does not lose its performance on the original tasks while learning the new one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKM0MekPzZ2p"
      },
      "source": [
        "### (e) Learning without Forgetting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUcyjEO2zKiF"
      },
      "source": [
        "\n",
        "This method seeks to train the model on a new task while retaining its performance on the previous tasks. It involves a mechanism that preserves the model's responses for the old tasks while learning the new task, typically by using knowledge distillation or a similar approach to retain the original model's output distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duiwUUw6y9pq"
      },
      "source": [
        "Each approach has its advantages and trade-offs. Fine-tuning and feature extraction are common strategies when you have limited data for the new task. Joint training and learning without forgetting are more sophisticated strategies that seek to maintain or improve performance across all tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvb_2f0OQSze"
      },
      "source": [
        "\n",
        "## Introduction to Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQmArQxQUzY"
      },
      "source": [
        "Transfer learning is akin to learning a new skill with the knowledge you already possess. It involves taking a model trained on one task and repurposing it for another related task. This method is not only efficient but also reduces the need for a large dataset, which is often a bottleneck in machine learning projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8huzBAXQaP5"
      },
      "source": [
        "\n",
        "### Why Transfer Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrgL353mQcgm"
      },
      "source": [
        "- **Efficiency**: Jumpstart your project by leveraging existing models that have already learned robust features from large datasets.\n",
        "- **Performance**: Pretrained models often yield surprisingly good results, even with a relatively small amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1hn4N72Vg7u"
      },
      "source": [
        "## Pre-trained Model Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkUuW6rQs3X"
      },
      "source": [
        "Here're the some sources that the pretrained models can be found:\n",
        "\n",
        "| Location | Link(s) |\n",
        "|----------|---------|\n",
        "| PyTorch domain libraries | [torchvision.models](https://pytorch.org/vision/stable/models.html), [torchtext.models](https://pytorch.org/text/stable/models.html), [torchaudio.models](https://pytorch.org/audio/stable/models.html), [torchrec.models](https://pytorch.org/torchrec/stable/models.html) |\n",
        "| HuggingFace Hub | [https://huggingface.co/models](https://huggingface.co/models), [https://huggingface.co/datasets](https://huggingface.co/datasets) |\n",
        "| timm (PyTorch Image Models) library | [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models) |\n",
        "| Paperswithcode | [https://paperswithcode.com/](https://paperswithcode.com/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK_3vjyNlEyG"
      },
      "source": [
        "## Transfer Learning with PyTorch's Updated API\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjxblXK1luu3"
      },
      "source": [
        "PyTorch's TorchVision library provides a plethora of models for tasks including image classification, segmentation, and object detection. However, the conventional API posed several limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQcC0FrlmH4"
      },
      "source": [
        "### Limitations of the Current API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAUZumylIEY"
      },
      "source": [
        "While TorchVision's `torchvision.models` offers a great starting point for transfer learning, it comes with limitations:\n",
        "\n",
        "- **Limited Pre-trained Weight Options**: The binary nature of the `pretrained` parameter restricts models to a single set of weights, impeding the adoption of improved or alternative pre-trained weights.\n",
        "- **Manual Inference Transform Definition**: Users must manually specify the preprocessing steps necessary for model inference, which can be error-prone and reduce model accuracy if done incorrectly.\n",
        "- **Lack of Metadata**: Essential information regarding the weights, such as category labels and training recipes, is not readily accessible, complicating model utilization and experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZZY9b6GlN-V"
      },
      "source": [
        "### Introducing the Prototype API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmYo0wSDl-5X"
      },
      "source": [
        "TorchVision addresses these challenges with a new prototype API, enhancing usability and flexibility. Let's explore the improvements offered by this API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVj6kluImGYU"
      },
      "source": [
        "### Step-by-Step Implementation with the New API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv_fVDwXmLwY",
        "outputId": "8fef7a55-62b6-4b66-a710-6c092b4aa0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category: 'seashore' \n",
            "Score 31.20%\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import models as PM\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Fetch the image from the URL\n",
        "url = \"https://raw.githubusercontent.com/arkeodev/pytorch-tutorial/main/Transfer_Learning/images/istanbul.jpg\"\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Initialize the model with pre-defined weights\n",
        "weights = PM.ResNet50_Weights.IMAGENET1K_V1\n",
        "model = PM.resnet50(weights=weights)\n",
        "model.eval()\n",
        "\n",
        "# Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Apply inference preprocessing transforms\n",
        "batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "# Use the model and print the predicted category\n",
        "prediction = model(batch).squeeze(0).softmax(0)\n",
        "class_id = prediction.argmax().item()\n",
        "score = prediction[class_id].item()\n",
        "category_name = weights.meta[\"categories\"][class_id]\n",
        "print(f\"Category: '{category_name}' \\nScore {100 * score:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5rMdEjwliYk"
      },
      "source": [
        "This approach simplifies model usage while addressing previous limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQjN-nYldyN"
      },
      "source": [
        "### Features of the New API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbxDJrG7mgsD"
      },
      "source": [
        "- **Multi-Weight Support**: By associating each model with an Enum class (e.g., `ResNet50_Weights`), the API now supports multiple sets of pre-trained weights.\n",
        "  \n",
        "- **Integrated Metadata and Preprocessing Transforms**: Each set of weights is linked with its corresponding metadata and preprocessing transforms, streamlining the model inference process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28VMsKk9mqFr"
      },
      "source": [
        "### Utilizing Different Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vHXYlxElazU"
      },
      "source": [
        "Here's how to use the API to select from different available weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g40mkIQ-mk2O",
        "outputId": "74231ba9-4ecc-442b-815e-b432d02d0518"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 142MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Using different sets of pre-trained weights\n",
        "model_a = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)  # Old weights\n",
        "model_b = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)  # Improved weights\n",
        "model_c = resnet50(weights=ResNet50_Weights.DEFAULT)        # Best available weights\n",
        "model_d = resnet50(weights=None)                            # No weights, random initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v04xOy_m0AI"
      },
      "source": [
        "### Accessing Metadata and Preprocessing Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwT7GwrSlXq_"
      },
      "source": [
        "The API makes it easy to access essential metadata and initialize the necessary preprocessing transforms for your data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehoy-avXmwl9",
        "outputId": "eaeb303f-4d19-4e00-d898-d3e861a147eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata:\n",
            "min_size: (1, 1)\n",
            "categories: ['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead', 'electric ray', 'stingray', 'cock', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'water ouzel', 'kite', 'bald eagle', 'vulture', 'great grey owl', 'European fire salamander', 'common newt', 'eft', 'spotted salamander', 'axolotl', 'bullfrog', 'tree frog', 'tailed frog', 'loggerhead', 'leatherback turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'common iguana', 'American chameleon', 'whiptail', 'agama', 'frilled lizard', 'alligator lizard', 'Gila monster', 'green lizard', 'African chameleon', 'Komodo dragon', 'African crocodile', 'American alligator', 'triceratops', 'thunder snake', 'ringneck snake', 'hognose snake', 'green snake', 'king snake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'rock python', 'Indian cobra', 'green mamba', 'sea snake', 'horned viper', 'diamondback', 'sidewinder', 'trilobite', 'harvestman', 'scorpion', 'black and gold garden spider', 'barn spider', 'garden spider', 'black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie chicken', 'peacock', 'quail', 'partridge', 'African grey', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'drake', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'American egret', 'bittern', 'crane bird', 'limpkin', 'European gallinule', 'American coot', 'bustard', 'ruddy turnstone', 'red-backed sandpiper', 'redshank', 'dowitcher', 'oystercatcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese spaniel', 'Maltese dog', 'Pekinese', 'Shih-Tzu', 'Blenheim spaniel', 'papillon', 'toy terrier', 'Rhodesian ridgeback', 'Afghan hound', 'basset', 'beagle', 'bloodhound', 'bluetick', 'black-and-tan coonhound', 'Walker hound', 'English foxhound', 'redbone', 'borzoi', 'Irish wolfhound', 'Italian greyhound', 'whippet', 'Ibizan hound', 'Norwegian elkhound', 'otterhound', 'Saluki', 'Scottish deerhound', 'Weimaraner', 'Staffordshire bullterrier', 'American Staffordshire terrier', 'Bedlington terrier', 'Border terrier', 'Kerry blue terrier', 'Irish terrier', 'Norfolk terrier', 'Norwich terrier', 'Yorkshire terrier', 'wire-haired fox terrier', 'Lakeland terrier', 'Sealyham terrier', 'Airedale', 'cairn', 'Australian terrier', 'Dandie Dinmont', 'Boston bull', 'miniature schnauzer', 'giant schnauzer', 'standard schnauzer', 'Scotch terrier', 'Tibetan terrier', 'silky terrier', 'soft-coated wheaten terrier', 'West Highland white terrier', 'Lhasa', 'flat-coated retriever', 'curly-coated retriever', 'golden retriever', 'Labrador retriever', 'Chesapeake Bay retriever', 'German short-haired pointer', 'vizsla', 'English setter', 'Irish setter', 'Gordon setter', 'Brittany spaniel', 'clumber', 'English springer', 'Welsh springer spaniel', 'cocker spaniel', 'Sussex spaniel', 'Irish water spaniel', 'kuvasz', 'schipperke', 'groenendael', 'malinois', 'briard', 'kelpie', 'komondor', 'Old English sheepdog', 'Shetland sheepdog', 'collie', 'Border collie', 'Bouvier des Flandres', 'Rottweiler', 'German shepherd', 'Doberman', 'miniature pinscher', 'Greater Swiss Mountain dog', 'Bernese mountain dog', 'Appenzeller', 'EntleBucher', 'boxer', 'bull mastiff', 'Tibetan mastiff', 'French bulldog', 'Great Dane', 'Saint Bernard', 'Eskimo dog', 'malamute', 'Siberian husky', 'dalmatian', 'affenpinscher', 'basenji', 'pug', 'Leonberg', 'Newfoundland', 'Great Pyrenees', 'Samoyed', 'Pomeranian', 'chow', 'keeshond', 'Brabancon griffon', 'Pembroke', 'Cardigan', 'toy poodle', 'miniature poodle', 'standard poodle', 'Mexican hairless', 'timber wolf', 'white wolf', 'red wolf', 'coyote', 'dingo', 'dhole', 'African hunting dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian cat', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'ice bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ladybug', 'ground beetle', 'long-horned beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket', 'walking stick', 'cockroach', 'mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'admiral', 'ringlet', 'monarch', 'cabbage butterfly', 'sulphur butterfly', 'lycaenid', 'starfish', 'sea urchin', 'sea cucumber', 'wood rabbit', 'hare', 'Angora', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'sorrel', 'zebra', 'hog', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram', 'bighorn', 'ibex', 'hartebeest', 'impala', 'gazelle', 'Arabian camel', 'llama', 'weasel', 'mink', 'polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas', 'baboon', 'macaque', 'langur', 'colobus', 'proboscis monkey', 'marmoset', 'capuchin', 'howler monkey', 'titi', 'spider monkey', 'squirrel monkey', 'Madagascar cat', 'indri', 'Indian elephant', 'African elephant', 'lesser panda', 'giant panda', 'barracouta', 'eel', 'coho', 'rock beauty', 'anemone fish', 'sturgeon', 'gar', 'lionfish', 'puffer', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'altar', 'ambulance', 'amphibian', 'analog clock', 'apiary', 'apron', 'ashcan', 'assault rifle', 'backpack', 'bakery', 'balance beam', 'balloon', 'ballpoint', 'Band Aid', 'banjo', 'bannister', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'barrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'bathing cap', 'bath towel', 'bathtub', 'beach wagon', 'beacon', 'beaker', 'bearskin', 'beer bottle', 'beer glass', 'bell cote', 'bib', 'bicycle-built-for-two', 'bikini', 'binder', 'binoculars', 'birdhouse', 'boathouse', 'bobsled', 'bolo tie', 'bonnet', 'bookcase', 'bookshop', 'bottlecap', 'bow', 'bow tie', 'brass', 'brassiere', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'bullet train', 'butcher shop', 'cab', 'caldron', 'candle', 'cannon', 'canoe', 'can opener', 'cardigan', 'car mirror', 'carousel', \"carpenter's kit\", 'carton', 'car wheel', 'cash machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'cellular telephone', 'chain', 'chainlink fence', 'chain mail', 'chain saw', 'chest', 'chiffonier', 'chime', 'china cabinet', 'Christmas stocking', 'church', 'cinema', 'cleaver', 'cliff dwelling', 'cloak', 'clog', 'cocktail shaker', 'coffee mug', 'coffeepot', 'coil', 'combination lock', 'computer keyboard', 'confectionery', 'container ship', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'crane', 'crash helmet', 'crate', 'crib', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishrag', 'dishwasher', 'disk brake', 'dock', 'dogsled', 'dome', 'doormat', 'drilling platform', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso maker', 'face powder', 'feather boa', 'file', 'fireboat', 'fire engine', 'fire screen', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gasmask', 'gas pump', 'goblet', 'go-kart', 'golf ball', 'golfcart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'grille', 'grocery store', 'guillotine', 'hair slide', 'hair spray', 'half track', 'hammer', 'hamper', 'hand blower', 'hand-held computer', 'handkerchief', 'hard disc', 'harmonica', 'harp', 'harvester', 'hatchet', 'holster', 'home theater', 'honeycomb', 'hook', 'hoopskirt', 'horizontal bar', 'horse cart', 'hourglass', 'iPod', 'iron', \"jack-o'-lantern\", 'jean', 'jeep', 'jersey', 'jigsaw puzzle', 'jinrikisha', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop', 'lawn mower', 'lens cap', 'letter opener', 'library', 'lifeboat', 'lighter', 'limousine', 'liner', 'lipstick', 'Loafer', 'lotion', 'loudspeaker', 'loupe', 'lumbermill', 'magnetic compass', 'mailbag', 'mailbox', 'maillot', 'maillot tank suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'matchstick', 'maypole', 'maze', 'measuring cup', 'medicine chest', 'megalith', 'microphone', 'microwave', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'Model T', 'modem', 'monastery', 'monitor', 'moped', 'mortar', 'mortarboard', 'mosque', 'mosquito net', 'motor scooter', 'mountain bike', 'mountain tent', 'mouse', 'mousetrap', 'moving van', 'muzzle', 'nail', 'neck brace', 'necklace', 'nipple', 'notebook', 'obelisk', 'oboe', 'ocarina', 'odometer', 'oil filter', 'organ', 'oscilloscope', 'overskirt', 'oxcart', 'oxygen mask', 'packet', 'paddle', 'paddlewheel', 'padlock', 'paintbrush', 'pajama', 'palace', 'panpipe', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'parking meter', 'passenger car', 'patio', 'pay-phone', 'pedestal', 'pencil box', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'pick', 'pickelhaube', 'picket fence', 'pickup', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pinwheel', 'pirate', 'pitcher', 'plane', 'planetarium', 'plastic bag', 'plate rack', 'plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'poncho', 'pool table', 'pop bottle', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'projector', 'puck', 'punching bag', 'purse', 'quill', 'quilt', 'racer', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'rubber eraser', 'rugby ball', 'rule', 'running shoe', 'safe', 'safety pin', 'saltshaker', 'sandal', 'sarong', 'sax', 'scabbard', 'scale', 'school bus', 'schooner', 'scoreboard', 'screen', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe shop', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski', 'ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar dish', 'sombrero', 'soup bowl', 'space bar', 'space heater', 'space shuttle', 'spatula', 'speedboat', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'steam locomotive', 'steel arch bridge', 'steel drum', 'stethoscope', 'stole', 'stone wall', 'stopwatch', 'stove', 'strainer', 'streetcar', 'stretcher', 'studio couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunglasses', 'sunscreen', 'suspension bridge', 'swab', 'sweatshirt', 'swimming trunks', 'swing', 'switch', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy', 'television', 'tennis ball', 'thatch', 'theater curtain', 'thimble', 'thresher', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toyshop', 'tractor', 'trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'trombone', 'tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright', 'vacuum', 'vase', 'vault', 'velvet', 'vending machine', 'vestment', 'viaduct', 'violin', 'volleyball', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'warplane', 'washbasin', 'washer', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'wing', 'wok', 'wooden spoon', 'wool', 'worm fence', 'wreck', 'yawl', 'yurt', 'web site', 'comic book', 'crossword puzzle', 'street sign', 'traffic light', 'book jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'trifle', 'ice cream', 'ice lolly', 'French loaf', 'bagel', 'pretzel', 'cheeseburger', 'hotdog', 'mashed potato', 'head cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'custard apple', 'pomegranate', 'hay', 'carbonara', 'chocolate sauce', 'dough', 'meat loaf', 'pizza', 'potpie', 'burrito', 'red wine', 'espresso', 'cup', 'eggnog', 'alp', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeside', 'promontory', 'sandbar', 'seashore', 'valley', 'volcano', 'ballplayer', 'groom', 'scuba diver', 'rapeseed', 'daisy', \"yellow lady's slipper\", 'corn', 'acorn', 'hip', 'buckeye', 'coral fungus', 'agaric', 'gyromitra', 'stinkhorn', 'earthstar', 'hen-of-the-woods', 'bolete', 'ear', 'toilet tissue']\n",
            "num_params: 25557032\n",
            "recipe: https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\n",
            "_metrics: {'ImageNet-1K': {'acc@1': 80.858, 'acc@5': 95.434}}\n",
            "_ops: 4.089\n",
            "_file_size: 97.79\n",
            "_docs: \n",
            "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
            "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
            "            \n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "# Initialize model with pre-defined weights\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "model.eval()\n",
        "\n",
        "# Access metadata\n",
        "metadata = models.ResNet50_Weights.IMAGENET1K_V2.meta\n",
        "\n",
        "# Print metadata in a formatted manner\n",
        "print(\"Metadata:\")\n",
        "for key, value in metadata.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP95Hu2Pm8oZ"
      },
      "source": [
        "### Retrieving Weights by Name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kXrkub1m-88"
      },
      "source": [
        "For situations where only the name of the weights is known, the API provides a convenient retrieval method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyg1CeWunB0J"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import get_weight\n",
        "\n",
        "# Retrieving weights by name\n",
        "weights = get_weight(\"ResNet50_Weights.IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz9XF1x1U__k"
      },
      "source": [
        "## Implementing Transfer Learning with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing transfer learning in PyTorch involves a few key steps:\n",
        "\n",
        "1. Loading a pre-trained model.\n",
        "2. Modifying the model for a new task.\n",
        "3. Defining a dataset for the new task.\n",
        "4. Training the model on the new task.\n",
        "5. Evaluating the model and making predictions.\n",
        "\n",
        "Let's go through each step in code."
      ],
      "metadata": {
        "id": "t7RdZvBXzAWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load and Prepare the Data"
      ],
      "metadata": {
        "id": "qUbzGMkizDYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the required packages"
      ],
      "metadata": {
        "id": "aWBtz9La0xVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "i7pARymV0qez"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to download the Hymenoptera dataset which is commonly used for transfer learning tasks.\n",
        "\n",
        "# Define the URLs for the training and validation datasets\n",
        "train_url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
        "train_filename = \"hymenoptera_data.zip\"\n",
        "\n",
        "# Create the data directories\n",
        "train_data_directory = \"./data/train\"\n",
        "test_data_directory = \"./data/test\"\n",
        "os.makedirs(train_data_directory, exist_ok=True)\n",
        "os.makedirs(test_data_directory, exist_ok=True)\n",
        "\n",
        "train_filepath = f\"{train_data_directory}/{train_filename}\"\n",
        "\n",
        "# Download the dataset (the download might take a few seconds)\n",
        "train_data = requests.get(train_url)\n",
        "\n",
        "# Save the downloaded zip file in the specified filepath\n",
        "with open(train_filepath, \"wb\") as train_file:\n",
        "    train_file.write(train_data.content)"
      ],
      "metadata": {
        "id": "Zf0D-jRu4nBp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by setting up the data loaders for our dataset using PyTorch's `DataLoader` class."
      ],
      "metadata": {
        "id": "Tcid7noRzNOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the required transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define the dataset using the ImageFolder\n",
        "train_dataset = ImageFolder(root=train_data_directory, transform=transform)\n",
        "test_dataset = ImageFolder(root=test_data_directory, transform=transform)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(len(train_dataset) * validation_split)\n",
        "num_training_samples = len(train_dataset) - num_validation_samples\n",
        "\n",
        "train_subset, validation_subset = torch.utils.data.random_split(\n",
        "    train_dataset, [num_training_samples, num_validation_samples]\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(validation_subset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "(train_loader, validation_loader, test_loader)"
      ],
      "metadata": {
        "id": "IBSP2Mu-zXjt",
        "outputId": "66fb6602-fbb9-45db-e44e-3b99a827fa86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in ./data/train.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-39f5be0b40d1>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Define the dataset using the ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in ./data/train."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Modify a Pre-Trained Model"
      ],
      "metadata": {
        "id": "MvkX5z8szZ0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load a pre-trained model and modify its classifier to fit our new task."
      ],
      "metadata": {
        "id": "xo5zkcF1zhaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# Modify the classifier for the new dataset\n",
        "num_features = model.fc.in_features\n",
        "model.fc = models.Linear(num_features, 2)"
      ],
      "metadata": {
        "id": "jA-k3T7Czoqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Train the Model"
      ],
      "metadata": {
        "id": "LvMeHHk0zsQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can define the training loop."
      ],
      "metadata": {
        "id": "6icLw5qYzsOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Number of epochs\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "8LkcR7ufz0lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Evaluate and Make Predictions"
      ],
      "metadata": {
        "id": "1HwagQR5z3Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we can evaluate the model on the validation set and make predictions."
      ],
      "metadata": {
        "id": "mpv6AQAzz5L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_inference_mode():\n",
        "    for inputs, labels in val_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {100 * correct // total}%\")"
      ],
      "metadata": {
        "id": "RjvH072Qz_y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ts-gMv_VJxk"
      },
      "source": [
        "## Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbNffEhQVLq7"
      },
      "source": [
        "Transfer learning opens up a realm of possibilities in deep learning projects. By utilizing pretrained models, you can accelerate development, conserve resources, and achieve remarkable results, even with limited data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueNczb_nwiKK"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPkUOg3nwl3h"
      },
      "source": [
        "- Li, Zhizhong, and Derek Hoiem. \"Learning without forgetting.\" In European Conference on Computer Vision, pp. 614-629. Springer International Publishing, 2016: https://arxiv.org/pdf/1606.09282.pdf\n",
        "\n",
        "- Introducing TorchVision’s New Multi-Weight Support API: https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}