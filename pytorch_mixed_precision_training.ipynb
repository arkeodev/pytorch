{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cuTb7EwViraP"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8Sad5HLZ1nsv/EtFn2GY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/pytorch-tutorial/blob/main/pytorch_mixed_precision_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizing PyTorch's GradScaler for Efficient Mixed Precision Training"
      ],
      "metadata": {
        "id": "dgrZs8RziNFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "cuTb7EwViraP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In recent years, mixed precision training has emerged as a key technique to accelerate deep learning model training without significantly impacting the model's accuracy. By utilizing both 16-bit (float16) and 32-bit (float32) floating-point arithmetic, mixed precision training reduces memory usage and speeds up computations on modern GPUs. PyTorch's `torch.cuda.amp` (Automatic Mixed Precision) package, particularly the `GradScaler` class, plays a pivotal role in facilitating this process."
      ],
      "metadata": {
        "id": "qEPLutPjiuwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Mixed Precision Training?"
      ],
      "metadata": {
        "id": "TNs1321nizxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed precision training leverages the strengths of both float16 and float32 data types.\n",
        "\n",
        "Float16 operations are faster and require less memory, enabling the training of larger models or increasing batch sizes. However, float16 can lead to issues like underflow and overflow in gradients, compromising training stability. Here's where float32 comes in, maintaining precision where necessary, especially during the calculation of loss and its subsequent gradients."
      ],
      "metadata": {
        "id": "EYXlxt4Ji1-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Overflow and Underflow Concepts"
      ],
      "metadata": {
        "id": "p0jHGeD1noBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding underflow and overflow in the context of floating-point arithmetic is crucial, especially when dealing with mixed precision training involving float16 (half precision) and float32 (single precision) formats. These issues are fundamental to why mixed precision training needs careful management, such as what `torch.cuda.amp.GradScaler` provides in PyTorch."
      ],
      "metadata": {
        "id": "E_kFo0z-nkyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overflow"
      ],
      "metadata": {
        "id": "4forxCVOn1gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overflow occurs when a number is too large to be represented in the given floating-point format. Each floating-point format has a maximum limit it can represent. When calculations exceed this limit, the result is typically set to an infinity value (`inf`), which can lead to incorrect calculations or model instability."
      ],
      "metadata": {
        "id": "DjzNeGhUn5I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example of Overflow"
      ],
      "metadata": {
        "id": "1ujFava4oGuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In float16, the maximum positive value that can be represented is approximately $(65504)$. If you attempt to multiply two large float16 numbers, say $(32000 \\times 3)$, the expected product would be $(96000)$, which exceeds the maximum representable value in float16 format.\n",
        "\n",
        "This results in an overflow, and the operation might yield infinity $(`inf`)$ instead of the actual number."
      ],
      "metadata": {
        "id": "jjNyFaI7oMHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical Python code illustrating the concept\n",
        "a = torch.tensor(32000, dtype=torch.float16)\n",
        "b = torch.tensor(3, dtype=torch.float16)\n",
        "product = a * b  # This could result in overflow in float16\n",
        "product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F6AmJw6oiKo",
        "outputId": "06f1b5d7-1a7f-404f-e5c5-9427c4655161"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(inf, dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Underflow"
      ],
      "metadata": {
        "id": "WWC1ifrloquQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underflow occurs when a number is too small to be represented in the given floating-point format, getting closer to zero than the format can accurately represent. This can result in the number being rounded down to zero. Underflow can significantly impact training by causing gradients to vanish, effectively stopping the model from learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "CMEaetFyotUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example of Underflow"
      ],
      "metadata": {
        "id": "Jt6x9sEbozs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In float16, the smallest positive number that can be represented is approximately $(6.1 \\times 10^{-5})$. If a gradient during backpropagation is calculated to be $(1.2 \\times 10^{-5})$, it is smaller than what float16 can represent.\n",
        "\n",
        "In this case, the gradient might be rounded down to $(0)$, leading to a vanishing gradient problem."
      ],
      "metadata": {
        "id": "1ZD6Ie_Ko39r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical Python code illustrating the concept\n",
        "small_gradient = torch.tensor(1.2e-5, dtype=torch.float16)\n",
        "# This could result in underflow, potentially becoming zero\n",
        "small_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgn24WWSpJCt",
        "outputId": "78ece04c-9fad-4fdc-934d-e76eb535783c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1981e-05, dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the theoretical minimum positive value $(around\\ 6.1 × 10^-5)$ is a helpful guideline, but the actual behavior can be influenced by:\n",
        "\n",
        " - Approximations: Floating-point representations themselves introduce slight inaccuracies.\n",
        "\n",
        "- Underflow handling: Hardware/software might return a pre-defined minimum value or a special indicator instead of zero during underflow."
      ],
      "metadata": {
        "id": "JOYnqsOryHus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Role of GradScaler"
      ],
      "metadata": {
        "id": "7Hbs_tlei5vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.cuda.amp.GradScaler` automatically adjusts the scale of the gradients, balancing between the speed of float16 and the precision of float32. This balancing act is crucial for preventing gradient underflow, ensuring that gradients are neither too small to vanish nor too large to cause overflow."
      ],
      "metadata": {
        "id": "JN3WH2r0i77h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Foundations\n",
        "\n"
      ],
      "metadata": {
        "id": "mtVpR23cjVh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core idea behind `GradScaler` is gradient scaling. Before backward propagation, loss values are scaled up by a factor `S` to prevent underflow. The gradients calculated during backward propagation are thus scaled up, and before the optimizer step, they are scaled back down by the same factor `S`.\n",
        "\n",
        "\n",
        "This process can be summarized as:\n",
        "\n",
        "1. Scale up: $(Loss_{scaled} = Loss \\times S)$\n",
        "2. Backward propagation: Compute gradients $(\\nabla Loss_{scaled})$\n",
        "3. Scale down: $(\\nabla Loss = \\nabla Loss_{scaled}\\ /\\ S)$"
      ],
      "metadata": {
        "id": "rwdJshHAjXSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Implementation"
      ],
      "metadata": {
        "id": "JJVkncQ1jyMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive into how you can implement mixed precision training with `GradScaler` in PyTorch."
      ],
      "metadata": {
        "id": "3xjctl0Yj_nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a Simple Model\n",
        "\n",
        "First, we define a simple convolutional neural network model suitable for the MNIST dataset:\n"
      ],
      "metadata": {
        "id": "2eRPEEd6k9Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(1024, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "PUMQYXafk_v4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Dataset\n"
      ],
      "metadata": {
        "id": "HUdO_SjZlL87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's load the MNIST dataset and prepare data loaders:"
      ],
      "metadata": {
        "id": "J6SqwydQlQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3zbu_qTlVWY",
        "outputId": "1caebb7f-dba1-4995-ca34-fe4f1bde22f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90693691.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 26431528.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29943201.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3219626.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model with Mixed Precision"
      ],
      "metadata": {
        "id": "6quF5sj_ljii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we incorporate the mixed precision training into the training loop:"
      ],
      "metadata": {
        "id": "0b-Zav4rll7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, optimizer, and loss function setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run model in mixed precision\n",
        "        with autocast():\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "        # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Scaler step. Updates the model parameters based on current gradients.\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # Updates the scale for next iteration.\n",
        "        scaler.update()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "Mjs-iJrIlsvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, the effectiveness and efficiency gains from mixed precision training can vary based on your model and hardware capabilities.\n",
        "\n",
        "Don't forget that, always monitor the model's performance and adjust the training setup as needed."
      ],
      "metadata": {
        "id": "0sGO6ZYuk04k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, `autocast` is used to automatically perform operations in float16 wherever beneficial, while `GradScaler` manages the scaling of gradients to prevent underflow."
      ],
      "metadata": {
        "id": "haFuLaYAj38S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "ZR-IERU7lzNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Mixed precision training is a powerful technique to enhance the performance and efficiency of training deep learning models. With `torch.cuda.amp.GradScaler`, PyTorch provides an accessible way to leverage this technique, ensuring that models can be trained faster without compromising on accuracy. Remember, the effectiveness of mixed precision training can vary depending on the model and hardware capabilities, so experimentation is key."
      ],
      "metadata": {
        "id": "BEHKTcF9h8ZI"
      }
    }
  ]
}