{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMch7FQciWvvusLj4keBWmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/pytorch-tutorial/blob/main/pytorch_lr_schedulers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Schedulers"
      ],
      "metadata": {
        "id": "5hNpn9uhXJwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition"
      ],
      "metadata": {
        "id": "4jwKGhheXOO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a learning rate schedule in PyTorch is a way to adjust the learning rate during training, which can help improve the performance of the model by controlling how weights are updated during the training process. This can be particularly useful for overcoming challenges like overfitting or speeding up the convergence.\n",
        "\n",
        "PyTorch provides several built-in schedulers through its torch.optim.lr_scheduler module. Here's a general approach to define and use a learning rate scheduler in PyTorch:\n",
        "\n",
        "1. Choose a Learning Rate Scheduler: First, decide which scheduler fits the needs. PyTorch offers several options, such as `StepLR`, `MultiStepLR`, `ExponentialLR`, `ReduceLROnPlateau`, and `CosineAnnealingLR`, among others. Each scheduler adjusts the learning rate according to its specific strategy.\n",
        "\n",
        "2. Define Optimizer: Before using a scheduler, it is needed an optimizer. The scheduler adjusts the learning rate for this optimizer.\n",
        "\n",
        "3. Instantiate the Scheduler: After defining the optimizer, it can be created an instance of the scheduler by passing it the optimizer and other parameters specific to the scheduler's strategy.\n",
        "\n",
        "4. Update Learning Rate During Training: In the training loop, it is updated the learning rate according to the scheduler's strategy. The method to update the learning rate depends on the scheduler. For some schedulers, it can be updated the learning rate at each epoch, while for others, it might be at each batch."
      ],
      "metadata": {
        "id": "odu30pRtYD_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Samples"
      ],
      "metadata": {
        "id": "mNNOqU-kXRHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StepLR\n"
      ],
      "metadata": {
        "id": "f2iAzwLrcfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `StepLR` scheduler decays the learning rate of each parameter group by a factor of `gamma` every `step_size` epochs. It's a simple yet effective way to decrease the learning rate over time.\n",
        "\n",
        "$$\n",
        "LR_{t} = LR_{0} \\cdot \\gamma^{\\left\\lfloor\\frac{epoch}{step\\_size}\\right\\rfloor}\n",
        "$$\n",
        "\n",
        "- **$(LR_{t})$**: The learning rate at epoch $(t)$.\n",
        "- **$(LR_{0})$**: The initial learning rate set at the beginning of training.\n",
        "- **$(\\gamma)$**: The factor by which the learning rate is multiplied at each step. It's a value between 0 and 1. A smaller value decays the learning rate more.\n",
        "- **$(epoch)$**: The current epoch number during the training process.\n",
        "- **$(step\\_size)$**: The frequency, in epochs, with which to multiply the learning rate by $(\\gamma)$.\n",
        "- **$(\\left\\lfloor\\frac{epoch}{step\\_size}\\right\\rfloor)$**: This denotes the floor division between the current epoch and the step size, effectively counting how many times the learning rate should have been updated. The learning rate is updated every time the division result increases.\n",
        "\n",
        "The formula describes how the learning rate decreases in discrete steps. Every $(step\\_size)$ epochs, the learning rate is multiplied by a factor of $(\\gamma)$, leading to a piecewise constant decay schedule."
      ],
      "metadata": {
        "id": "uz3KDqc4XhcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Sample model\n",
        "model = nn.Linear(10, 2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    # Dummy training step\n",
        "    optimizer.zero_grad()\n",
        "    output = model(torch.randn(5, 10))\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Current LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGQqYmjuZTLd",
        "outputId": "1d717399-5725-4f56-a424-b8db276819cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Current LR: 1.00e-01\n",
            "Epoch 2, Current LR: 1.00e-01\n",
            "Epoch 3, Current LR: 1.00e-01\n",
            "Epoch 4, Current LR: 1.00e-01\n",
            "Epoch 5, Current LR: 1.00e-01\n",
            "Epoch 6, Current LR: 1.00e-01\n",
            "Epoch 7, Current LR: 1.00e-01\n",
            "Epoch 8, Current LR: 1.00e-01\n",
            "Epoch 9, Current LR: 1.00e-01\n",
            "Epoch 10, Current LR: 1.00e-01\n",
            "Epoch 11, Current LR: 1.00e-02\n",
            "Epoch 12, Current LR: 1.00e-02\n",
            "Epoch 13, Current LR: 1.00e-02\n",
            "Epoch 14, Current LR: 1.00e-02\n",
            "Epoch 15, Current LR: 1.00e-02\n",
            "Epoch 16, Current LR: 1.00e-02\n",
            "Epoch 17, Current LR: 1.00e-02\n",
            "Epoch 18, Current LR: 1.00e-02\n",
            "Epoch 19, Current LR: 1.00e-02\n",
            "Epoch 20, Current LR: 1.00e-02\n",
            "Epoch 21, Current LR: 1.00e-03\n",
            "Epoch 22, Current LR: 1.00e-03\n",
            "Epoch 23, Current LR: 1.00e-03\n",
            "Epoch 24, Current LR: 1.00e-03\n",
            "Epoch 25, Current LR: 1.00e-03\n",
            "Epoch 26, Current LR: 1.00e-03\n",
            "Epoch 27, Current LR: 1.00e-03\n",
            "Epoch 28, Current LR: 1.00e-03\n",
            "Epoch 29, Current LR: 1.00e-03\n",
            "Epoch 30, Current LR: 1.00e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiStepLR"
      ],
      "metadata": {
        "id": "7gbB6uU2cmbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MultiStepLR` decays the learning rate of each parameter group by `gamma` once the number of epochs reaches one of the milestones. It offers more flexibility than `StepLR` by allowing for non-uniform step sizes.\n",
        "\n",
        "$$\n",
        "LR_{t} = LR_{0} \\cdot \\gamma^{\\sum_{m \\in milestones} \\mathbb{1}_{epoch > m}}\n",
        "$$\n",
        "\n",
        "- **$(\\sum_{m \\in milestones} \\mathbb{1}_{epoch > m})$**: This summation counts the number of milestones that have been passed. The function $(\\mathbb{1}_{epoch > m})$ is an indicator function that returns 1 if the current epoch is greater than the milestone $(m)$ and 0 otherwise.\n",
        "- **$(milestones)$**: A set of epoch numbers at which the learning rate should be decreased.\n",
        "\n",
        "Similar to `StepLR`, `MultiStepLR` reduces the learning rate by multiplying it with $(\\gamma)$, but it does so whenever the current epoch surpasses any of the predefined milestones, allowing for more flexibility compared to the regular step decay."
      ],
      "metadata": {
        "id": "9M-KA0ghXnWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Sample model\n",
        "model = nn.Linear(10, 2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[15, 25], gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    # Dummy training step\n",
        "    optimizer.zero_grad()\n",
        "    output = model(torch.randn(5, 10))\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Current LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzb-gVLZcueQ",
        "outputId": "7d6e6767-9ca4-49c2-9b52-38dcaaae00df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Current LR: 1.00e-01\n",
            "Epoch 2, Current LR: 1.00e-01\n",
            "Epoch 3, Current LR: 1.00e-01\n",
            "Epoch 4, Current LR: 1.00e-01\n",
            "Epoch 5, Current LR: 1.00e-01\n",
            "Epoch 6, Current LR: 1.00e-01\n",
            "Epoch 7, Current LR: 1.00e-01\n",
            "Epoch 8, Current LR: 1.00e-01\n",
            "Epoch 9, Current LR: 1.00e-01\n",
            "Epoch 10, Current LR: 1.00e-01\n",
            "Epoch 11, Current LR: 1.00e-01\n",
            "Epoch 12, Current LR: 1.00e-01\n",
            "Epoch 13, Current LR: 1.00e-01\n",
            "Epoch 14, Current LR: 1.00e-01\n",
            "Epoch 15, Current LR: 1.00e-01\n",
            "Epoch 16, Current LR: 1.00e-02\n",
            "Epoch 17, Current LR: 1.00e-02\n",
            "Epoch 18, Current LR: 1.00e-02\n",
            "Epoch 19, Current LR: 1.00e-02\n",
            "Epoch 20, Current LR: 1.00e-02\n",
            "Epoch 21, Current LR: 1.00e-02\n",
            "Epoch 22, Current LR: 1.00e-02\n",
            "Epoch 23, Current LR: 1.00e-02\n",
            "Epoch 24, Current LR: 1.00e-02\n",
            "Epoch 25, Current LR: 1.00e-02\n",
            "Epoch 26, Current LR: 1.00e-03\n",
            "Epoch 27, Current LR: 1.00e-03\n",
            "Epoch 28, Current LR: 1.00e-03\n",
            "Epoch 29, Current LR: 1.00e-03\n",
            "Epoch 30, Current LR: 1.00e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExponentialLR"
      ],
      "metadata": {
        "id": "2LyvccQnc0jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ExponentialLR` decays the learning rate of each parameter group by a factor of `gamma` every epoch. It provides a smooth, exponential decrease in the learning rate.\n",
        "\n",
        "$$\n",
        "LR_{t} = LR_{0} \\cdot \\gamma^{epoch}\n",
        "$$\n",
        "\n",
        "This formula provides a smooth, exponential decrease in the learning rate over time:\n",
        "\n",
        "- **$(\\gamma^{epoch})$**: The exponential decay factor, raised to the power of the current epoch. The learning rate is exponentially decreased by this factor every epoch.\n",
        "\n",
        "\n",
        "`ExponentialLR` ensures a smooth and continuous decay of the learning rate, making it suitable for fine-grained adjustments over the course of training."
      ],
      "metadata": {
        "id": "-p-a-C4-XtQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Sample model\n",
        "model = nn.Linear(10, 2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    # Dummy training step\n",
        "    optimizer.zero_grad()\n",
        "    output = model(torch.randn(5, 10))\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Current LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG4d2mpXc6sX",
        "outputId": "9a8666de-cc8e-4e2e-f32a-60f0775d7882"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Current LR: 1.00e-01\n",
            "Epoch 2, Current LR: 9.50e-02\n",
            "Epoch 3, Current LR: 9.02e-02\n",
            "Epoch 4, Current LR: 8.57e-02\n",
            "Epoch 5, Current LR: 8.15e-02\n",
            "Epoch 6, Current LR: 7.74e-02\n",
            "Epoch 7, Current LR: 7.35e-02\n",
            "Epoch 8, Current LR: 6.98e-02\n",
            "Epoch 9, Current LR: 6.63e-02\n",
            "Epoch 10, Current LR: 6.30e-02\n",
            "Epoch 11, Current LR: 5.99e-02\n",
            "Epoch 12, Current LR: 5.69e-02\n",
            "Epoch 13, Current LR: 5.40e-02\n",
            "Epoch 14, Current LR: 5.13e-02\n",
            "Epoch 15, Current LR: 4.88e-02\n",
            "Epoch 16, Current LR: 4.63e-02\n",
            "Epoch 17, Current LR: 4.40e-02\n",
            "Epoch 18, Current LR: 4.18e-02\n",
            "Epoch 19, Current LR: 3.97e-02\n",
            "Epoch 20, Current LR: 3.77e-02\n",
            "Epoch 21, Current LR: 3.58e-02\n",
            "Epoch 22, Current LR: 3.41e-02\n",
            "Epoch 23, Current LR: 3.24e-02\n",
            "Epoch 24, Current LR: 3.07e-02\n",
            "Epoch 25, Current LR: 2.92e-02\n",
            "Epoch 26, Current LR: 2.77e-02\n",
            "Epoch 27, Current LR: 2.64e-02\n",
            "Epoch 28, Current LR: 2.50e-02\n",
            "Epoch 29, Current LR: 2.38e-02\n",
            "Epoch 30, Current LR: 2.26e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "a1pKlSqLdDA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ReduceLROnPlateau` reduces the learning rate when a metric has stopped improving, offering a way to fine-tune models when using validation metrics.\n",
        "\n",
        "Reduces the learning rate by `factor` if the selected metric does not improve for a `patience` number of epochs.\n",
        "\n",
        "Its principle can be summarized as follows:\n",
        "\n",
        "- If the metric (e.g., validation loss) does not improve for a specified number of epochs $(patience)$, the learning rate is reduced by a factor of $(factor)$.\n",
        "\n",
        "This approach helps to fine-tune the model by reducing the learning rate when progress stalls, potentially aiding in escaping local minima or making fine adjustments as the model approaches convergence."
      ],
      "metadata": {
        "id": "_wSF_1Z8XzgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)\n",
        "        self.fc2 = nn.Linear(5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleModel()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define ReduceLROnPlateau scheduler\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10)\n",
        "\n",
        "# Mock training and validation loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    # Simulate a training step. Normally, it should be:\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Simulated training loss (decreases)\n",
        "    train_loss = 1.0 / (0.1 * epoch + 1)\n",
        "\n",
        "    # Simulate a validation step\n",
        "    model.eval()\n",
        "\n",
        "    # Simulated validation loss (random fluctuation around 0.5)\n",
        "    val_loss = 0.5 + 0.05 * np.sin(epoch)\n",
        "\n",
        "    # Print current epoch, training loss, validation loss, and current learning rate\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Current LR: {scheduler.optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    # Update the learning rate based on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Note: This example is entirely fictional and designed to demonstrate how to use the ReduceLROnPlateau scheduler.\n",
        "# The losses are simulated and do not reflect actual training and validation processes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_brwPxAdIEk",
        "outputId": "77602ffe-2f51-495e-d6f2-fd1eb4f9b248"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.0000, Val Loss: 0.5000, Current LR: 1.00e-01\n",
            "Epoch 2, Train Loss: 0.9091, Val Loss: 0.5421, Current LR: 1.00e-01\n",
            "Epoch 3, Train Loss: 0.8333, Val Loss: 0.5455, Current LR: 1.00e-01\n",
            "Epoch 4, Train Loss: 0.7692, Val Loss: 0.5071, Current LR: 1.00e-01\n",
            "Epoch 5, Train Loss: 0.7143, Val Loss: 0.4622, Current LR: 1.00e-01\n",
            "Epoch 6, Train Loss: 0.6667, Val Loss: 0.4521, Current LR: 1.00e-01\n",
            "Epoch 7, Train Loss: 0.6250, Val Loss: 0.4860, Current LR: 1.00e-01\n",
            "Epoch 8, Train Loss: 0.5882, Val Loss: 0.5328, Current LR: 1.00e-01\n",
            "Epoch 9, Train Loss: 0.5556, Val Loss: 0.5495, Current LR: 1.00e-01\n",
            "Epoch 10, Train Loss: 0.5263, Val Loss: 0.5206, Current LR: 1.00e-01\n",
            "Epoch 11, Train Loss: 0.5000, Val Loss: 0.4728, Current LR: 1.00e-01\n",
            "Epoch 12, Train Loss: 0.4762, Val Loss: 0.4500, Current LR: 1.00e-01\n",
            "Epoch 13, Train Loss: 0.4545, Val Loss: 0.4732, Current LR: 1.00e-01\n",
            "Epoch 14, Train Loss: 0.4348, Val Loss: 0.5210, Current LR: 1.00e-01\n",
            "Epoch 15, Train Loss: 0.4167, Val Loss: 0.5495, Current LR: 1.00e-01\n",
            "Epoch 16, Train Loss: 0.4000, Val Loss: 0.5325, Current LR: 1.00e-01\n",
            "Epoch 17, Train Loss: 0.3846, Val Loss: 0.4856, Current LR: 1.00e-01\n",
            "Epoch 18, Train Loss: 0.3704, Val Loss: 0.4519, Current LR: 1.00e-01\n",
            "Epoch 19, Train Loss: 0.3571, Val Loss: 0.4625, Current LR: 1.00e-01\n",
            "Epoch 20, Train Loss: 0.3448, Val Loss: 0.5075, Current LR: 1.00e-01\n",
            "Epoch 21, Train Loss: 0.3333, Val Loss: 0.5456, Current LR: 1.00e-01\n",
            "Epoch 22, Train Loss: 0.3226, Val Loss: 0.5418, Current LR: 1.00e-01\n",
            "Epoch 23, Train Loss: 0.3125, Val Loss: 0.4996, Current LR: 1.00e-01\n",
            "Epoch 24, Train Loss: 0.3030, Val Loss: 0.4577, Current LR: 1.00e-02\n",
            "Epoch 25, Train Loss: 0.2941, Val Loss: 0.4547, Current LR: 1.00e-02\n",
            "Epoch 26, Train Loss: 0.2857, Val Loss: 0.4934, Current LR: 1.00e-02\n",
            "Epoch 27, Train Loss: 0.2778, Val Loss: 0.5381, Current LR: 1.00e-02\n",
            "Epoch 28, Train Loss: 0.2703, Val Loss: 0.5478, Current LR: 1.00e-02\n",
            "Epoch 29, Train Loss: 0.2632, Val Loss: 0.5135, Current LR: 1.00e-02\n",
            "Epoch 30, Train Loss: 0.2564, Val Loss: 0.4668, Current LR: 1.00e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CosineAnnealingLR"
      ],
      "metadata": {
        "id": "878WwP2ldP8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CosineAnnealingLR` provides a cosine annealing schedule, where the learning rate decreases following a cosine curve between an initial lr set by the optimizer and a minimum lr, over a given number of epochs.\n",
        "\n",
        "$$\n",
        "LR_{t} = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
        "$$\n",
        "\n",
        "- **$(\\eta_{min})$** and **$(\\eta_{max})$**: The minimum and maximum boundaries for the learning rate.\n",
        "- **$(T_{cur})$**: The current epoch.\n",
        "- **$(T_{max})$**: The maximum number of epochs.\n",
        "\n",
        "This scheduler decreases the learning rate following a cosine curve between $(\\eta_{max})$ and $(\\eta_{min})$, simulating a restart by returning to $(\\eta_{max})$ after each cycle (of length $(T_{max}))$.\n",
        "\n",
        "This pattern helps to navigate the loss landscape by periodically resetting the learning rate, potentially avoiding local minima and encouraging exploration of the parameter space."
      ],
      "metadata": {
        "id": "ol37fmyWXUUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Sample model\n",
        "model = nn.Linear(10, 2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    # Dummy training step\n",
        "    optimizer.zero_grad()\n",
        "    output = model(torch.randn(5, 10))\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Current LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5qzUiDwdTeY",
        "outputId": "9ca00625-e185-4ea1-8f57-e5ed97fd7e58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Current LR: 1.00e-01\n",
            "Epoch 2, Current LR: 9.99e-02\n",
            "Epoch 3, Current LR: 9.96e-02\n",
            "Epoch 4, Current LR: 9.91e-02\n",
            "Epoch 5, Current LR: 9.84e-02\n",
            "Epoch 6, Current LR: 9.76e-02\n",
            "Epoch 7, Current LR: 9.65e-02\n",
            "Epoch 8, Current LR: 9.52e-02\n",
            "Epoch 9, Current LR: 9.38e-02\n",
            "Epoch 10, Current LR: 9.22e-02\n",
            "Epoch 11, Current LR: 9.05e-02\n",
            "Epoch 12, Current LR: 8.85e-02\n",
            "Epoch 13, Current LR: 8.64e-02\n",
            "Epoch 14, Current LR: 8.42e-02\n",
            "Epoch 15, Current LR: 8.19e-02\n",
            "Epoch 16, Current LR: 7.94e-02\n",
            "Epoch 17, Current LR: 7.68e-02\n",
            "Epoch 18, Current LR: 7.41e-02\n",
            "Epoch 19, Current LR: 7.13e-02\n",
            "Epoch 20, Current LR: 6.84e-02\n",
            "Epoch 21, Current LR: 6.55e-02\n",
            "Epoch 22, Current LR: 6.24e-02\n",
            "Epoch 23, Current LR: 5.94e-02\n",
            "Epoch 24, Current LR: 5.63e-02\n",
            "Epoch 25, Current LR: 5.31e-02\n",
            "Epoch 26, Current LR: 5.00e-02\n",
            "Epoch 27, Current LR: 4.69e-02\n",
            "Epoch 28, Current LR: 4.37e-02\n",
            "Epoch 29, Current LR: 4.06e-02\n",
            "Epoch 30, Current LR: 3.76e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "xxHFV4MnX3Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each scheduler is suited to different scenarios and requirements. Experimentation is key to finding the most effective scheduler and parameters for the specific problem and dataset."
      ],
      "metadata": {
        "id": "mUM6UDM2X6zn"
      }
    }
  ]
}