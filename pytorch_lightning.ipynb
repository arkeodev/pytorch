{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/pytorch-tutorial/blob/main/pytorch_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9fde7e",
      "metadata": {
        "id": "5e9fde7e"
      },
      "source": [
        "# Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df83c04f",
      "metadata": {
        "id": "df83c04f"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning is a high-level framework that builds on top of PyTorch, one of the most popular deep learning libraries. It's designed to decouple the science code from the engineering code, helping researchers and developers focus on the core aspects of their models by abstracting away the boilerplate code typically associated with model training, validation, and testing. This approach not only makes the code more readable and maintainable but also significantly speeds up the development process for complex deep learning projects."
      ],
      "metadata": {
        "id": "VlH9H6QTJjD5"
      },
      "id": "VlH9H6QTJjD5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Advantages of Using Lightning Over Plain PyTorch"
      ],
      "metadata": {
        "id": "QmO4nSgCEKES"
      },
      "id": "QmO4nSgCEKES"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Reduced Boilerplate Code**: Lightning automates much of the setup code needed in PyTorch, such as training loops, validation loops, and testing loops, allowing developers to focus on the model's architecture and data rather than the mechanics of the training process.\n",
        "\n",
        "2. **Reproducibility**: It ensures experiments are more reproducible by standardizing the way models are trained. This is achieved through a structured framework that encourages best practices and reduces the chances of making errors.\n",
        "\n",
        "3. **Scalability**: Lightning simplifies the process of scaling your models to run on more GPUs, TPUs, or across multiple nodes. This makes it easier to scale your experiments without having to deeply understand distributed computing.\n",
        "\n",
        "4. **Flexibility**: Despite the high-level abstractions, Lightning offers flexibility, allowing advanced users to customize the training loop and other components when needed. This means you can start with the simple, high-level interface and dive deeper as your project's complexity grows.\n",
        "\n",
        "5. **Built-in Advanced Features**: Lightning comes with many advanced features out of the box, such as support for mixed precision training, which can significantly speed up computations and reduce memory usage, and automatic checkpointing, which makes it easy to save and resume training sessions.\n",
        "\n",
        "6. **Community and Ecosystem**: Lightning has a vibrant and growing community, with a wide range of plugins and integrations available. This ecosystem includes support for popular tools and platforms, making it easier to incorporate things like logging, monitoring, and model serving into your workflow.\n",
        "\n",
        "In summary, Lightning is designed to make deep learning projects simpler, faster, and more efficient, without sacrificing the power and flexibility that PyTorch provides. By abstracting away the engineering details, it enables researchers and developers to allocate more time to the scientific aspects of their projects, resulting in faster experimentation and development cycles."
      ],
      "metadata": {
        "id": "sjJZzsEpJKvW"
      },
      "id": "sjJZzsEpJKvW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "O7YPSaRN_aCy"
      },
      "id": "O7YPSaRN_aCy"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install lightning -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDedC1Y0_dUE",
        "outputId": "b361cb3d-6ee3-4e63-d744-e10ba49c0bd4"
      },
      "id": "mDedC1Y0_dUE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m912.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, DistributedSampler, random_split\n",
        "import torch.distributed as dist\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "ieV9lcyQ_-Bc"
      },
      "id": "ieV9lcyQ_-Bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d19ef3c6",
      "metadata": {
        "id": "d19ef3c6"
      },
      "source": [
        "## Core Concepts of Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightningModule"
      ],
      "metadata": {
        "id": "dRTW-kgGKaHq"
      },
      "id": "dRTW-kgGKaHq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LightningModule` is a central concept in Lightning, acting as a comprehensive encapsulation of the PyTorch `nn.Module`. It serves as the backbone for organizing your model's computations, including the forward pass, and it also integrates the training, validation, and testing steps within a single class. This approach significantly simplifies the model development process by structuring the code in a way that separates the computational part of the model from the experimental setup.\n",
        "\n",
        "A `LightningModule` defines:\n",
        "- **Model Architecture**: How the inputs are processed to produce outputs, encapsulated in the `forward` method.\n",
        "- **Training Step**: The logic for a single iteration in the training loop, including forward pass, loss calculation, and backpropagation.\n",
        "- **Validation and Testing Steps**: Procedures for evaluating the model on validation and test datasets to monitor performance and prevent overfitting.\n",
        "- **Optimizers and Schedulers**: Configuration of optimizers and learning rate schedulers, specifying how weights are updated and how the learning rate changes over time.\n",
        "\n",
        "By integrating these aspects into a unified class, `LightningModule` streamlines model development, making the code more modular, easier to read, and maintain, while also promoting best practices in deep learning research and development."
      ],
      "metadata": {
        "id": "wTe1llG-KNqe"
      },
      "id": "wTe1llG-KNqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "86hHiKMQKXDs"
      },
      "id": "86hHiKMQKXDs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Trainer` in Lightning is a powerful engine that abstracts the complexity of writing the training loop and integrates your PyTorch code with the rich ecosystem of Lightning features. It is responsible for managing the training process, including running the training, validation, and testing loops, handling device placement (CPU, GPU, TPU), and facilitating distributed training.\n",
        "\n",
        "Key features of the `Trainer` include:\n",
        "- **Automatic Training Loop**: It automates the training process, managing everything from the start of training to its conclusion, including calling the appropriate steps defined in the `LightningModule`.\n",
        "- **Checkpointing**: Automatically saves and, if needed, resumes the model's state from a checkpoint, ensuring long experiments can be paused and restarted without loss of progress.\n",
        "- **Logging and Monitoring**: Integrates with popular logging and visualization tools (e.g., TensorBoard, MLFlow), enabling easy tracking of experiments and model performance.\n",
        "- **Distributed Training**: Simplifies scaling up your training to multiple GPUs, TPUs, or nodes without the need to deeply understand the underlying distributed computing frameworks."
      ],
      "metadata": {
        "id": "wuYxOrGNKQPi"
      },
      "id": "wuYxOrGNKQPi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataModule"
      ],
      "metadata": {
        "id": "Cwwwdtg3KUQS"
      },
      "id": "Cwwwdtg3KUQS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `DataModule` is a data handling class that abstracts the complexity of data loading, preparation, and preprocessing in Lightning. It allows for a clean separation of data-related logic from the modeling code, making datasets reusable and shareable across projects.\n",
        "\n",
        "A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
        "- Download / tokenize / process\n",
        "- Clean and (maybe) save to disk.\n",
        "- Load inside Dataset.\n",
        "- Apply transforms (rotate, tokenize, etc…).\n",
        "- Wrap inside a DataLoader.\n",
        "\n",
        "By encapsulating data-related tasks, the `DataModule` promotes a more organized and modular approach to handling datasets in PyTorch projects, making it easier to adapt to new data sources or experiment with different preprocessing techniques."
      ],
      "metadata": {
        "id": "ovmk_pbfKILt"
      },
      "id": "ovmk_pbfKILt"
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(L.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = \"./\"):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\":\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(\n",
        "                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
        "            )\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\":\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "        if stage == \"predict\":\n",
        "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=32)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=32)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=32)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.mnist_predict, batch_size=32)"
      ],
      "metadata": {
        "id": "eLSaukkHxxdZ"
      },
      "id": "eLSaukkHxxdZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Lightning, the flow of execution for data-related methods within a `LightningDataModule` follows a specific order, particularly when you start training a model. Understanding this order can help you design your data module more effectively. Based on the implementation you provided and the typical lifecycle of a PyTorch Lightning training process, here's the order in which these methods are called:\n",
        "\n",
        "1. **`prepare_data`:** This method is called first and only once globally. It's designed for operations that need to be done once per dataset, such as downloading or preparing data. In a distributed setting, this ensures that the data is not redundantly downloaded by each process. Lightning takes care to call this method only on one process when running in distributed modes.\n",
        "\n",
        "2. **`setup`:** After `prepare_data`, the `setup` method is called. This method is executed once per process. If you're running your model on multiple GPUs or nodes, `setup` will be called separately in each process. This method is where you should split your dataset and apply any transformations or preprocessing steps that are necessary. The `setup` method can be called with an optional `stage` argument (such as 'fit', 'validate', 'test', or 'predict'), allowing you to customize setup behavior depending on the training phase. If no `stage` is specified, it's assumed that the setup is for training.\n",
        "\n",
        "3. **DataLoader methods (`train_dataloader`, `val_dataloader`, `test_dataloader`, `predict_dataloader`):** These methods are called after `setup` and right before the corresponding phase begins (e.g., training, validation, testing). They are responsible for returning the PyTorch `DataLoader` instances that will be used to load data during the model training or inference. These methods can be called multiple times throughout the lifecycle of training and evaluation, especially if the training process includes multiple epochs."
      ],
      "metadata": {
        "id": "BSi7eozCOUj3"
      },
      "id": "BSi7eozCOUj3"
    },
    {
      "cell_type": "markdown",
      "id": "6010c9fc",
      "metadata": {
        "id": "6010c9fc"
      },
      "source": [
        "## Key Features of Lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Automation of Common Training Procedures"
      ],
      "metadata": {
        "id": "P7cwLphJKmbg"
      },
      "id": "P7cwLphJKmbg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning\n"
      ],
      "metadata": {
        "id": "PCbZXKJG9V3e"
      },
      "id": "PCbZXKJG9V3e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In standard PyTorch, setting up a training loop requires manually coding the entire process, including forward passes, calculating loss, backpropagation, and updating model parameters."
      ],
      "metadata": {
        "id": "0McjqDnW9aWs"
      },
      "id": "0McjqDnW9aWs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(28 * 28, 128)\n",
        "        self.layer2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = nn.functional.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize network and optimizer\n",
        "model = SimpleNet()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10): # train for 10 epochs\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYBpfDYu9gSY",
        "outputId": "1f1b7927-2814-4de7-a253-d8b90f94aef2"
      },
      "id": "uYBpfDYu9gSY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 148260703.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22650653.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 132227432.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14323705.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Epoch 0, Loss: 0.14844422042369843\n",
            "Epoch 1, Loss: 0.1093560978770256\n",
            "Epoch 2, Loss: 0.10502023249864578\n",
            "Epoch 3, Loss: 0.19714826345443726\n",
            "Epoch 4, Loss: 0.010195491835474968\n",
            "Epoch 5, Loss: 0.13798223435878754\n",
            "Epoch 6, Loss: 0.02335546910762787\n",
            "Epoch 7, Loss: 0.23562780022621155\n",
            "Epoch 8, Loss: 0.004139881581068039\n",
            "Epoch 9, Loss: 0.005697409622371197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning"
      ],
      "metadata": {
        "id": "jWvnln1O_EAx"
      },
      "id": "jWvnln1O_EAx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning automates the training process, requiring you to define the training step, and it handles the rest."
      ],
      "metadata": {
        "id": "k-sFePoa_GhM"
      },
      "id": "k-sFePoa_GhM"
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningNet(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(LightningNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(28 * 28, 128)\n",
        "        self.layer2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = nn.functional.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        output = self(images)\n",
        "        loss = nn.CrossEntropyLoss()(output, labels)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "# Data module for organizing data loading\n",
        "class MNISTDataModule(L.LightningDataModule):\n",
        "    def train_dataloader(self):\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "        train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "        return DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# Setup model and data module\n",
        "model = LightningNet()\n",
        "mnist_data = MNISTDataModule()\n",
        "\n",
        "# Train model\n",
        "trainer = L.Trainer(max_epochs=10)\n",
        "trainer.fit(model, mnist_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888,
          "referenced_widgets": [
            "20a44ae1a6e64fb2857a1dffa03e0b46",
            "e3a4a65bc6304a11b37893b2e6816767",
            "7a0c01b165a1428d90afc1fe758b4378",
            "ed1e942ff742430e89a485361332eb22",
            "c9ef252196ea4b67bcc695251c0a622d",
            "10e123d1613a41789e65961d482af9cb",
            "4e26e6e727264321829bd8b0fe8e184f",
            "1cfaf86535da47b2b24fdf0fd01b613a",
            "3cf327c1c44c4c69a3b70cedca4cbb47",
            "57cfa6f8b1bd4b31bfee23c1bad0f9c9",
            "7500c083795f4ba1ac20c4f1e08f0bc7"
          ]
        },
        "id": "asRBdiwv_Po5",
        "outputId": "2c525c81-61c0-4309-cb14-659c3537a572"
      },
      "id": "asRBdiwv_Po5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING: Missing logger folder: /content/lightning_logs\n",
            "WARNING:lightning.pytorch.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO: \n",
            "  | Name   | Type   | Params\n",
            "----------------------------------\n",
            "0 | layer1 | Linear | 100 K \n",
            "1 | layer2 | Linear | 1.3 K \n",
            "----------------------------------\n",
            "101 K     Trainable params\n",
            "0         Non-trainable params\n",
            "101 K     Total params\n",
            "0.407     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name   | Type   | Params\n",
            "----------------------------------\n",
            "0 | layer1 | Linear | 100 K \n",
            "1 | layer2 | Linear | 1.3 K \n",
            "----------------------------------\n",
            "101 K     Trainable params\n",
            "0         Non-trainable params\n",
            "101 K     Total params\n",
            "0.407     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 95724665.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 36084508.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29697997.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6137412.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a44ae1a6e64fb2857a1dffa03e0b46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison"
      ],
      "metadata": {
        "id": "_aIBs7qsA4J9"
      },
      "id": "_aIBs7qsA4J9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Automation and Simplification**: With Lightning, the training process is simplified as you only need to define the `training_step` and the `configure_optimizers` method. The `Trainer` object then automates the training loop, including forward and backward passes, optimization, and more.\n",
        "\n",
        "- **Reduction of Boilerplate Code**: Lightning significantly reduces the amount of boilerplate code required, especially in training loops. This makes the code more readable and easier to maintain.\n",
        "\n",
        "- **Focus on Model and Data**: Lightning encourages a separation of concerns, allowing you to focus on the model (`LightningModule`) and data (`DataModule`) separately. This results in cleaner, more modular code that's easier to debug and extend.\n",
        "\n",
        "- **Built-in Best Practices**: Lightning incorporates many best practices by default, such as gradient clipping and logging, reducing the chance of common mistakes and improving the efficiency of the development process."
      ],
      "metadata": {
        "id": "HdGxT6hC9J0x"
      },
      "id": "HdGxT6hC9J0x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Reproducibility\n"
      ],
      "metadata": {
        "id": "HhBvwwoCLW6J"
      },
      "id": "HhBvwwoCLW6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning\n",
        "\n"
      ],
      "metadata": {
        "id": "LHFF0NNLBw26"
      },
      "id": "LHFF0NNLBw26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When not using Lightning, you would manually set seeds for all the relevant libraries you are using to ensure reproducibility. This often includes PyTorch itself, NumPy (if used for data manipulation), and Python’s random module."
      ],
      "metadata": {
        "id": "z8xyyP48By7L"
      },
      "id": "z8xyyP48By7L"
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # For CUDA-enabled GPUs\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "z_4fXsoYB3Qc"
      },
      "id": "z_4fXsoYB3Qc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning\n"
      ],
      "metadata": {
        "id": "xZUS3_MSCIfY"
      },
      "id": "xZUS3_MSCIfY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning provides a straightforward way to fix seeds across all the necessary libraries with a single line of code, ensuring reproducibility across runs."
      ],
      "metadata": {
        "id": "9s7KpkBUCKt0"
      },
      "id": "9s7KpkBUCKt0"
    },
    {
      "cell_type": "code",
      "source": [
        "L.seed_everything(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxmLwfFVCRYa",
        "outputId": "05d44341-420d-44ac-aefd-0f5384c8d723"
      },
      "id": "nxmLwfFVCRYa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 42\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Easy Experiment Tracking"
      ],
      "metadata": {
        "id": "KQAms_9pCrqF"
      },
      "id": "KQAms_9pCrqF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning"
      ],
      "metadata": {
        "id": "ho0uwuLmCuhb"
      },
      "id": "ho0uwuLmCuhb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracking experiments without Lightning often requires manually logging metrics, model configurations, and other details to a file, a database, or a tool like TensorBoard. This can quickly become cumbersome and error-prone as the complexity of the experiments grows."
      ],
      "metadata": {
        "id": "nt9ccu2cCxQW"
      },
      "id": "nt9ccu2cCxQW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(10):  # Example training loop\n",
        "    # Training steps\n",
        "    # ...\n",
        "    writer.add_scalar('Loss/train', loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
        "\n",
        "writer.close()\n",
        "```"
      ],
      "metadata": {
        "id": "h41-Dfb2C3Xq"
      },
      "id": "h41-Dfb2C3Xq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning"
      ],
      "metadata": {
        "id": "IHW3KYwvC68l"
      },
      "id": "IHW3KYwvC68l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning simplifies experiment tracking and integrates seamlessly with popular tools like TensorBoard, MLFlow, Comet ML, and others. By using loggers, you can easily record metrics, hyperparameters, model graphs, and more without cluttering your model code."
      ],
      "metadata": {
        "id": "7IJFseRqDEz5"
      },
      "id": "7IJFseRqDEz5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "# Define a logger\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
        "\n",
        "# Train model with logger\n",
        "trainer = Trainer(logger=logger, max_epochs=10)\n",
        "trainer.fit(model)\n",
        "\n",
        "# Access the experiment URL if using an online logger like MLFlow, Comet ML, etc.\n",
        "print(\"Experiment URL:\", logger.experiment.url)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PP6PyT_BBrIa"
      },
      "id": "PP6PyT_BBrIa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Out-of-the-Box Support for Multi-GPU, TPU, and Distributed Training"
      ],
      "metadata": {
        "id": "FYJ4q3EqL3IK"
      },
      "id": "FYJ4q3EqL3IK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning"
      ],
      "metadata": {
        "id": "N7jvQe_dEaks"
      },
      "id": "N7jvQe_dEaks"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing multi-GPU, TPU, or distributed training in PyTorch requires a deep understanding of the underlying mechanisms like `torch.nn.DataParallel`, `torch.nn.parallel.DistributedDataParallel`, or setting up TPU environments. This process can be complex and error-prone, especially for those new to distributed computing concepts."
      ],
      "metadata": {
        "id": "qDaE4KNnEdGp"
      },
      "id": "qDaE4KNnEdGp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    # Model definition\n",
        "\n",
        "def train(rank, world_size):\n",
        "    setup(rank, world_size)\n",
        "    # Model, optimizer, data loader setup\n",
        "    model = MyModel()\n",
        "    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "    # Training loop\n",
        "    cleanup()\n",
        "\n",
        "world_size = 2\n",
        "torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
        "```"
      ],
      "metadata": {
        "id": "lkZGabIwEm4e"
      },
      "id": "lkZGabIwEm4e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning"
      ],
      "metadata": {
        "id": "1cCJ_eLJE2SS"
      },
      "id": "1cCJ_eLJE2SS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning simplifies the process, allowing you to easily scale your models across multiple GPUs, TPUs, or nodes with minimal changes to your code. The `Trainer` class handles the complexity of distributed training.\n",
        "\n",
        "`DDP Communication hooks` is an interface to control how gradients are communicated across workers, overriding the standard allreduce in `DistributedDataParallel`. This allows you to enable performance improving communication hooks when using multiple nodes. Enable `FP16 Compress Hook` for multi-node throughput improvement:\n"
      ],
      "metadata": {
        "id": "qUPg97FeE5-w"
      },
      "id": "qUPg97FeE5-w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import lightning as L\n",
        "from lightning.pytorch.strategies import DDPStrategy\n",
        "from torch.distributed.algorithms.ddp_comm_hooks import default_hooks as default\n",
        "\n",
        "model = MyModel()\n",
        "trainer = L.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=4,  # Number of GPUs\n",
        "    strategy=DDPStrategy(ddp_comm_hook=default.fp16_compress_hook)\n",
        ")\n",
        "\n",
        "trainer.fit(model)\n",
        "```"
      ],
      "metadata": {
        "id": "qS9_ldtrFE06"
      },
      "id": "qS9_ldtrFE06"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Integration with Popular Computing Resources and Environments"
      ],
      "metadata": {
        "id": "3znmYAbgFHyS"
      },
      "id": "3znmYAbgFHyS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning"
      ],
      "metadata": {
        "id": "M_vjPzH_FNDL"
      },
      "id": "M_vjPzH_FNDL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up your PyTorch models to run on cloud platforms (e.g., AWS, GCP, Azure) or specialized computing environments (e.g., HPC clusters) often requires a significant amount of boilerplate code and configuration. This includes managing environments, dependencies, data storage, and compute resources."
      ],
      "metadata": {
        "id": "xkcCiqFJFPK-"
      },
      "id": "xkcCiqFJFPK-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning"
      ],
      "metadata": {
        "id": "XJZRnVOYFRkw"
      },
      "id": "XJZRnVOYFRkw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning offers integrations with a variety of computing resources and environments, streamlining the process of deploying and running your models. Whether it's on a cloud provider or an HPC cluster, Lightning's Trainer and its ecosystem are designed to work seamlessly with minimal configuration."
      ],
      "metadata": {
        "id": "LOiAXdhmFWcB"
      },
      "id": "LOiAXdhmFWcB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Automatic Checkpointing and Resuming of Training"
      ],
      "metadata": {
        "id": "qt0scixoMhNs"
      },
      "id": "qt0scixoMhNs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Without Lightning"
      ],
      "metadata": {
        "id": "rUuT8X_WGQy7"
      },
      "id": "rUuT8X_WGQy7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing automatic checkpointing and resuming in plain PyTorch requires manually saving the model and optimizer state at specific intervals and then writing additional code to load these checkpoints if training is interrupted or needs to be resumed later."
      ],
      "metadata": {
        "id": "gVyaaqArGVID"
      },
      "id": "gVyaaqArGVID"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import torch\n",
        "\n",
        "# Save checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, filepath):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, filepath)\n",
        "\n",
        "# Load checkpoint\n",
        "def load_checkpoint(filepath, model, optimizer):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return checkpoint['epoch']\n",
        "\n",
        "# Example usage\n",
        "model = ...\n",
        "optimizer = ...\n",
        "epoch_start = load_checkpoint('path_to_checkpoint.pt', model, optimizer)\n",
        "\n",
        "for epoch in range(epoch_start, num_epochs):\n",
        "    # Training loop...\n",
        "    save_checkpoint(model, optimizer, epoch, 'path_to_checkpoint.pt')\n",
        "```"
      ],
      "metadata": {
        "id": "FBurpzwNGY8W"
      },
      "id": "FBurpzwNGY8W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning"
      ],
      "metadata": {
        "id": "XJRfrY2eGbsD"
      },
      "id": "XJRfrY2eGbsD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning simplifies checkpointing and resuming. The `Trainer` class has built-in support for checkpointing, automatically saving the model, optimizer, and training state at specified intervals or based on performance metrics. Resuming is as simple as providing the checkpoint path when initializing the trainer."
      ],
      "metadata": {
        "id": "EZt_fOv1GfEb"
      },
      "id": "EZt_fOv1GfEb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "# Setup model checkpointing\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss', # Monitor validation loss for checkpointing\n",
        "    dirpath='my_model/',\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=3, # Save the top 3 models according to val_loss\n",
        "    mode='min', # Minimize val_loss\n",
        ")\n",
        "\n",
        "# Initialize trainer with checkpoint callback\n",
        "trainer = Trainer(\n",
        "    callbacks=[checkpoint_callback],\n",
        "    resume_from_checkpoint='my_model/model-epoch=02-val_loss=0.02.ckpt', # Optional: Path to resume from a specific checkpoint\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = ...\n",
        "trainer.fit(model)\n",
        "```"
      ],
      "metadata": {
        "id": "ogssV5gtGDA7"
      },
      "id": "ogssV5gtGDA7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Built-in Support for Mixed-Precision Training\n"
      ],
      "metadata": {
        "id": "8vzWIDpVGs39"
      },
      "id": "8vzWIDpVGs39"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Lightning"
      ],
      "metadata": {
        "id": "6eZLa8JgGuay"
      },
      "id": "6eZLa8JgGuay"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In vanilla PyTorch, enabling mixed-precision training involves using `torch.cuda.amp` for automatic mixed precision (AMP), which can reduce memory usage and speed up training times on compatible hardware. This requires manual management of the AMP context.\n"
      ],
      "metadata": {
        "id": "UN-yYdj4Gzlc"
      },
      "id": "UN-yYdj4Gzlc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "model = ...\n",
        "optimizer = ...\n",
        "scaler = GradScaler()\n",
        "\n",
        "for inputs, labels in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    with autocast():\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "```\n"
      ],
      "metadata": {
        "id": "e6nydydfG2zN"
      },
      "id": "e6nydydfG2zN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Lightning\n"
      ],
      "metadata": {
        "id": "rA8teHlBG6Hd"
      },
      "id": "rA8teHlBG6Hd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning offers a straightforward way to enable mixed-precision training with a single flag in the `Trainer`. This automatically handles the AMP context and scaler under the hood."
      ],
      "metadata": {
        "id": "rgaRcexLG8LY"
      },
      "id": "rgaRcexLG8LY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from lightning.pytorch import Trainer\n",
        "\n",
        "# Enable mixed precision training\n",
        "trainer = Trainer(precision=16)\n",
        "\n",
        "# Train model\n",
        "model = ...\n",
        "trainer.fit(model)\n",
        "```"
      ],
      "metadata": {
        "id": "wGj3ALTtGJyq"
      },
      "id": "wGj3ALTtGJyq"
    },
    {
      "cell_type": "markdown",
      "id": "fd27d5a9",
      "metadata": {
        "id": "fd27d5a9"
      },
      "source": [
        "## Setting Up a Basic Lightning Project\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a basic project with Lightning involves a few straightforward steps that help streamline the development process for deep learning models. Here's a guide to get you started:"
      ],
      "metadata": {
        "id": "IESXUD8RIEue"
      },
      "id": "IESXUD8RIEue"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Install Lightning"
      ],
      "metadata": {
        "id": "5OSZaoo1IU8q"
      },
      "id": "5OSZaoo1IU8q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install lightning -q"
      ],
      "metadata": {
        "id": "6FSQauPhIgIk"
      },
      "id": "6FSQauPhIgIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Define Your Model"
      ],
      "metadata": {
        "id": "uHHzQoL9I1ri"
      },
      "id": "uHHzQoL9I1ri"
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Create a Python file (e.g., `model.py`) and define your model by subclassing `L.LightningModule`.\n",
        "   \n",
        "   Implement the required methods such as `__init__`, `forward`, `training_step`, and `configure_optimizers`."
      ],
      "metadata": {
        "id": "Atf0UD0-I6cW"
      },
      "id": "Atf0UD0-I6cW"
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define model layers\n",
        "        self.layer = nn.Linear(28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        x = x.view(x.size(0), -1)  # Reshape input to [batch_size, 784]\n",
        "        return torch.relu(self.layer(x))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Training logic\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.functional.cross_entropy(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Optimizers\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "zBaykWkAJECy"
      },
      "id": "zBaykWkAJECy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Prepare Your Data"
      ],
      "metadata": {
        "id": "xO5G509oIM9J"
      },
      "id": "xO5G509oIM9J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Define your data using PyTorch's `DataLoader` or Lightning's `DataModule`.\n",
        "   \n",
        "   A `DataModule` is a shareable, reusable class that encapsulates all data loading logic."
      ],
      "metadata": {
        "id": "ECq6kTsPJV9p"
      },
      "id": "ECq6kTsPJV9p"
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(L.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download only once\n",
        "        MNIST('data', train=True, download=True)\n",
        "        MNIST('data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Transform and split data\n",
        "        mnist_full = MNIST('data', train=True, transform=self.transform)\n",
        "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "        self.mnist_test = MNIST('data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=32)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=32)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=32)"
      ],
      "metadata": {
        "id": "8evxBq5TJa_Q"
      },
      "id": "8evxBq5TJa_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Train Your Model"
      ],
      "metadata": {
        "id": "ujJ80UeoIRnI"
      },
      "id": "ujJ80UeoIRnI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Use the `Trainer` class to train your model. Specify any configurations like the number of epochs, GPUs, etc."
      ],
      "metadata": {
        "id": "3_zwbVRwJ-bp"
      },
      "id": "3_zwbVRwJ-bp"
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch import Trainer\n",
        "\n",
        "model = LitModel()\n",
        "mnist_data = MNISTDataModule()\n",
        "trainer = Trainer(max_epochs=2)\n",
        "trainer.fit(model, mnist_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "a6b7dfae3b204158bc0890792e8b9103",
            "229c83f7270e48d9b6874ef5b57bc02c",
            "285eba7c1e214575a5fa36aec26108f2",
            "ff258fce6dad4534ae0f10c9beae1591",
            "bafe057d9b9b4986b0f7bfc9c79d7c5a",
            "80463a9bc0c045109c72c7a509469181",
            "8339dd0b81da4ead98847cace4981157",
            "4e5c74ec863942168e5f5573c15a0c2c",
            "bb610ca24dfc4359bd5596afd343d9e5",
            "d46b7d05622341cfb0ed23bb693851f3",
            "9128627f9e1a4f01832b76ed3d35985a"
          ]
        },
        "id": "etANVuPMKEU4",
        "outputId": "a686a109-0bba-4421-c836-6a138668ed87"
      },
      "id": "etANVuPMKEU4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "INFO: \n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | layer | Linear | 7.9 K \n",
            "---------------------------------\n",
            "7.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "7.9 K     Total params\n",
            "0.031     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | layer | Linear | 7.9 K \n",
            "---------------------------------\n",
            "7.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "7.9 K     Total params\n",
            "0.031     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6b7dfae3b204158bc0890792e8b9103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57573c3e",
      "metadata": {
        "id": "57573c3e"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning streamlines deep learning development by abstracting boilerplate code, enforcing best practices, and simplifying complexity. It supports scalable training across multiple GPUs and TPUs effortlessly, enhances reproducibility with features like fixed seeds, and remains flexible for custom needs. The active community and rich ecosystem provide extensive resources and support, making Lightning a powerful tool for efficient and reliable deep learning projects."
      ],
      "metadata": {
        "id": "4Owg4JctPSCM"
      },
      "id": "4Owg4JctPSCM"
    },
    {
      "cell_type": "markdown",
      "id": "3348edeb",
      "metadata": {
        "id": "3348edeb"
      },
      "source": [
        "## Resources and Further Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comprehensive information and resources on Lightning, here are the key places to look:\n",
        "\n",
        "- **Documentation and Tutorials**: The [official documentation](https://lightning.ai/docs/pytorch/stable/) is a comprehensive resource for getting started with Lightning, offering detailed guides, API references, and tutorials for users of all levels.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DhUYVRG0Lxw8"
      },
      "id": "DhUYVRG0Lxw8"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20a44ae1a6e64fb2857a1dffa03e0b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3a4a65bc6304a11b37893b2e6816767",
              "IPY_MODEL_7a0c01b165a1428d90afc1fe758b4378",
              "IPY_MODEL_ed1e942ff742430e89a485361332eb22"
            ],
            "layout": "IPY_MODEL_c9ef252196ea4b67bcc695251c0a622d"
          }
        },
        "e3a4a65bc6304a11b37893b2e6816767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e123d1613a41789e65961d482af9cb",
            "placeholder": "​",
            "style": "IPY_MODEL_4e26e6e727264321829bd8b0fe8e184f",
            "value": "Epoch 9: 100%"
          }
        },
        "7a0c01b165a1428d90afc1fe758b4378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cfaf86535da47b2b24fdf0fd01b613a",
            "max": 1875,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cf327c1c44c4c69a3b70cedca4cbb47",
            "value": 1875
          }
        },
        "ed1e942ff742430e89a485361332eb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57cfa6f8b1bd4b31bfee23c1bad0f9c9",
            "placeholder": "​",
            "style": "IPY_MODEL_7500c083795f4ba1ac20c4f1e08f0bc7",
            "value": " 1875/1875 [00:32&lt;00:00, 56.88it/s, v_num=0]"
          }
        },
        "c9ef252196ea4b67bcc695251c0a622d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "10e123d1613a41789e65961d482af9cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e26e6e727264321829bd8b0fe8e184f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cfaf86535da47b2b24fdf0fd01b613a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf327c1c44c4c69a3b70cedca4cbb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57cfa6f8b1bd4b31bfee23c1bad0f9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7500c083795f4ba1ac20c4f1e08f0bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6b7dfae3b204158bc0890792e8b9103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_229c83f7270e48d9b6874ef5b57bc02c",
              "IPY_MODEL_285eba7c1e214575a5fa36aec26108f2",
              "IPY_MODEL_ff258fce6dad4534ae0f10c9beae1591"
            ],
            "layout": "IPY_MODEL_bafe057d9b9b4986b0f7bfc9c79d7c5a"
          }
        },
        "229c83f7270e48d9b6874ef5b57bc02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80463a9bc0c045109c72c7a509469181",
            "placeholder": "​",
            "style": "IPY_MODEL_8339dd0b81da4ead98847cace4981157",
            "value": "Epoch 1: 100%"
          }
        },
        "285eba7c1e214575a5fa36aec26108f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e5c74ec863942168e5f5573c15a0c2c",
            "max": 1719,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb610ca24dfc4359bd5596afd343d9e5",
            "value": 1719
          }
        },
        "ff258fce6dad4534ae0f10c9beae1591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46b7d05622341cfb0ed23bb693851f3",
            "placeholder": "​",
            "style": "IPY_MODEL_9128627f9e1a4f01832b76ed3d35985a",
            "value": " 1719/1719 [00:26&lt;00:00, 64.64it/s, v_num=1]"
          }
        },
        "bafe057d9b9b4986b0f7bfc9c79d7c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "80463a9bc0c045109c72c7a509469181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8339dd0b81da4ead98847cace4981157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e5c74ec863942168e5f5573c15a0c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb610ca24dfc4359bd5596afd343d9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d46b7d05622341cfb0ed23bb693851f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9128627f9e1a4f01832b76ed3d35985a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}